{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7tsjK98ARox124m5Asz64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliasNoorzad/XAI_Autonomous-Driving/blob/main/evaluation/03_tritask_cbam_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEjSZ5kDX2Y2",
        "outputId": "f77e2297-7481-4813-8905-c489a5706c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics==8.4.6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azkbx-Y7aYlX",
        "outputId": "38a92d73-a511-4f39-c84b-1ae589a13651"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/XAI_Project/BDD100K_640.zip /content/"
      ],
      "metadata": {
        "id": "oTkdhsYXX3Zc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/BDD100K_640.zip -d /content/BDD100K_640"
      ],
      "metadata": {
        "id": "5YWJZvLnX67N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/XAI_Project/daynight_labels.csv /content/"
      ],
      "metadata": {
        "id": "PaWGNelYa_3T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml = \"\"\"\\\n",
        "path: /content/BDD100K_640/yolo_640\n",
        "train: train/images\n",
        "val: val/images\n",
        "test: test/images\n",
        "\n",
        "nc: 5\n",
        "names: [car, truck, bus, person, bike]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/BDD100K_640/yolo_640/dataset_640.yaml\", \"w\") as f:\n",
        "    f.write(yaml)"
      ],
      "metadata": {
        "id": "wrWOTs-oX_Dt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class BDDDetDrivableDataset(Dataset):\n",
        "    \"\"\"\n",
        "    For preprocessed 640 dataset (yolo_640 + drivable_masks_640):\n",
        "      images: <yolo_root>/<split>/images/<stem>.jpg\n",
        "      labels: <yolo_root>/<split>/labels/<stem>.txt   (already corrected for 640)\n",
        "      masks : <mask_root>/<split>/<stem>.png          (already 640)\n",
        "\n",
        "    Returns:\n",
        "      img   : FloatTensor [3, H, W] in [0,1]\n",
        "      labels: FloatTensor [N, 5] where each row is [cls, x, y, w, h] (YOLO normalized)\n",
        "      mask  : FloatTensor [1, H, W] with values 0/1\n",
        "      dn    : LongTensor scalar (0=day, 1=night)  <-- always valid (unlabeled images are filtered out)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        yolo_root: str,\n",
        "        mask_root: str,\n",
        "        split: str,\n",
        "        imgsz: int = 640,\n",
        "        dn_csv_path: str | None = None,\n",
        "    ):\n",
        "        self.yolo_root = Path(yolo_root)\n",
        "        self.mask_root = Path(mask_root)\n",
        "        self.split = split\n",
        "        self.imgsz = int(imgsz)\n",
        "\n",
        "        self.img_dir = self.yolo_root / split / \"images\"\n",
        "        self.lbl_dir = self.yolo_root / split / \"labels\"\n",
        "        self.msk_dir = self.mask_root / split\n",
        "\n",
        "        if not self.img_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing images dir: {self.img_dir}\")\n",
        "        if not self.lbl_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing labels dir: {self.lbl_dir}\")\n",
        "        if not self.msk_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing masks dir:  {self.msk_dir}\")\n",
        "\n",
        "        exts = {\".jpg\", \".jpeg\", \".png\"}\n",
        "        self.img_paths = sorted([p for p in self.img_dir.iterdir() if p.suffix.lower() in exts])\n",
        "        if len(self.img_paths) == 0:\n",
        "            raise FileNotFoundError(f\"No images found in: {self.img_dir}\")\n",
        "\n",
        "        # ---------- day/night mapping from CSV (REQUIRED for Stage B) ----------\n",
        "        if dn_csv_path is None:\n",
        "            raise RuntimeError(\"dn_csv_path is required for this dataset (day/night head training).\")\n",
        "\n",
        "        dn_csv_path = Path(dn_csv_path)\n",
        "        if not dn_csv_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing day/night CSV: {dn_csv_path}\")\n",
        "\n",
        "        dn_map = {}\n",
        "        with open(dn_csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            required = {\"split\", \"image_id\", \"label\"}\n",
        "            if not required.issubset(set(reader.fieldnames or [])):\n",
        "                raise ValueError(f\"dn CSV must have columns {required}, got {reader.fieldnames}\")\n",
        "\n",
        "            for row in reader:\n",
        "                if row[\"split\"] != self.split:\n",
        "                    continue\n",
        "\n",
        "                image_id = row[\"image_id\"].strip()   # stem (no extension)\n",
        "                lab = row[\"label\"].strip().lower()\n",
        "\n",
        "                if lab == \"day\":\n",
        "                    dn = 0\n",
        "                elif lab == \"night\":\n",
        "                    dn = 1\n",
        "                else:\n",
        "                    # if CSV contains anything else, it's a data error\n",
        "                    raise ValueError(f\"Invalid dn label in CSV for {image_id}: {row['label']}\")\n",
        "\n",
        "                dn_map[image_id] = dn\n",
        "\n",
        "        self.dn_map = dn_map\n",
        "\n",
        "        # ---------- FILTER OUT UNLABELED IMAGES ----------\n",
        "        before = len(self.img_paths)\n",
        "        self.img_paths = [p for p in self.img_paths if p.stem in self.dn_map]\n",
        "        after = len(self.img_paths)\n",
        "        if after == 0:\n",
        "            raise RuntimeError(f\"No labeled (day/night) images found for split='{self.split}'.\")\n",
        "        # --------------------------------------------------\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_yolo_labels(label_path: Path) -> torch.Tensor:\n",
        "        if not label_path.exists():\n",
        "            return torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "        rows = []\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                cls, x, y, w, h = parts\n",
        "                rows.append([float(cls), float(x), float(y), float(w), float(h)])\n",
        "\n",
        "        if len(rows) == 0:\n",
        "            return torch.zeros((0, 5), dtype=torch.float32)\n",
        "        return torch.tensor(rows, dtype=torch.float32)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pil_to_chw_float(img: Image.Image) -> torch.Tensor:\n",
        "        arr = np.array(img, dtype=np.float32) / 255.0\n",
        "        arr = np.transpose(arr, (2, 0, 1))\n",
        "        return torch.from_numpy(arr)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        img_path = self.img_paths[idx]\n",
        "        stem = img_path.stem\n",
        "\n",
        "        label_path = self.lbl_dir / f\"{stem}.txt\"\n",
        "        mask_path = self.msk_dir / f\"{stem}.png\"\n",
        "\n",
        "        if not mask_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing mask for {stem}: {mask_path}\")\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        labels = self._read_yolo_labels(label_path)\n",
        "\n",
        "        if img.size != (self.imgsz, self.imgsz):\n",
        "            img = img.resize((self.imgsz, self.imgsz), resample=Image.BILINEAR)\n",
        "        if mask.size != (self.imgsz, self.imgsz):\n",
        "            mask = mask.resize((self.imgsz, self.imgsz), resample=Image.NEAREST)\n",
        "\n",
        "        img_t = self._pil_to_chw_float(img)\n",
        "        mask_np = (np.array(mask, dtype=np.uint8) > 0).astype(np.float32)\n",
        "        mask_t = torch.from_numpy(mask_np)[None, :, :]\n",
        "\n",
        "        # dn is ALWAYS valid because we filtered img_paths\n",
        "        dn = torch.tensor(self.dn_map[stem], dtype=torch.long)\n",
        "\n",
        "        return img_t, labels, mask_t, dn\n"
      ],
      "metadata": {
        "id": "LgD-no7jYi7s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 16):\n",
        "        super().__init__()\n",
        "        hidden = max(channels // reduction, 1)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # shared MLP (implemented with 1x1 convs)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(channels, hidden, kernel_size=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, channels, kernel_size=1, bias=False),\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        avg_out = self.mlp(self.avg_pool(x))\n",
        "        max_out = self.mlp(self.max_pool(x))\n",
        "        w = self.sigmoid(avg_out + max_out)  # BxCx1x1\n",
        "        return x * w\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size: int = 7):\n",
        "        super().__init__()\n",
        "        assert kernel_size in (3, 7)\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.last_sa = None  # <--- add this\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        mean_map = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        m = torch.cat([mean_map, max_map], dim=1)\n",
        "\n",
        "        w = self.sigmoid(self.conv(m))  # Bx1xHxW in [0,1]\n",
        "        self.last_sa = w.detach()\n",
        "        return x * w\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 16, spatial_kernel: int = 7):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
        "        self.sa = SpatialAttention(kernel_size=spatial_kernel)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.ca(x)\n",
        "        x = self.sa(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "X2nWN0iCYngU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "x = torch.randn(2, 128, 80, 80)\n",
        "m = CBAM(channels=128, reduction=16, spatial_kernel=7)\n",
        "\n",
        "y = m(x)\n",
        "\n",
        "assert y.shape == x.shape, (x.shape, y.shape)\n",
        "assert not torch.isnan(y).any(), \"NaNs found in CBAM output\"\n",
        "print(\"CBAM sanity OK:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW-InhwHYs2K",
        "outputId": "18715bc5-fd64-4a79-af2f-9c4e5e6be077"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBAM sanity OK: torch.Size([2, 128, 80, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "class YOLOv8DetSemSeg(nn.Module):\n",
        "    \"\"\"\n",
        "    YOLOv8 detection model + tiny semantic seg head.\n",
        "    Captures NECK features by hooking the Detect head INPUT (multi-scale features).\n",
        "    \"\"\"\n",
        "    def __init__(self, yolo_weights: str = \"yolov8n.pt\", use_cbam: bool = False):\n",
        "        super().__init__()\n",
        "        self.yolo = YOLO(yolo_weights).model  # nn.Module\n",
        "        self.use_cbam = use_cbam\n",
        "\n",
        "        self.cbam_backbone = None  # Point 1 (after last backbone block)\n",
        "        self.cbam_neck = None      # Point 2 (CBAM on ALL neck feature maps)\n",
        "\n",
        "        self._neck_feats = None\n",
        "\n",
        "        self.sem_head = nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.dn_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),  # [B,C,1,1]\n",
        "            nn.Flatten(1),            # [B,C]\n",
        "            nn.LazyLinear(2)          # [B,2] logits: [day, night]\n",
        "        )\n",
        "\n",
        "        self._register_backbone_hook_point1()\n",
        "        self._register_detect_input_hook()\n",
        "\n",
        "    def _register_backbone_hook_point1(self):\n",
        "        idx_up = None\n",
        "        for i, m in enumerate(self.yolo.model):\n",
        "            if isinstance(m, nn.Upsample) or \"upsample\" in m.__class__.__name__.lower():\n",
        "                idx_up = i\n",
        "                break\n",
        "        if idx_up is None or idx_up == 0:\n",
        "            raise RuntimeError(\"Could not find neck start (Upsample) to place backbone CBAM.\")\n",
        "\n",
        "        backbone_last = self.yolo.model[idx_up - 1]\n",
        "\n",
        "        if hasattr(self, \"_bb_hook_handle\") and self._bb_hook_handle is not None:\n",
        "            self._bb_hook_handle.remove()\n",
        "\n",
        "        def fwd_hook(module, inputs, output):\n",
        "            if not self.use_cbam:\n",
        "                return None\n",
        "            if self.cbam_backbone is None:\n",
        "                self.cbam_backbone = CBAM(channels=output.shape[1]).to(output.device)\n",
        "            return self.cbam_backbone(output)\n",
        "\n",
        "        self._bb_hook_handle = backbone_last.register_forward_hook(fwd_hook)\n",
        "\n",
        "    def _register_detect_input_hook(self):\n",
        "        if not hasattr(self.yolo, \"model\"):\n",
        "            raise RuntimeError(\"Unexpected Ultralytics model: no .model\")\n",
        "\n",
        "        detect_module = self.yolo.model[-1]\n",
        "        name = detect_module.__class__.__name__.lower()\n",
        "        if \"detect\" not in name:\n",
        "            raise RuntimeError(f\"Last module is not Detect (got {detect_module.__class__.__name__}).\")\n",
        "\n",
        "        if hasattr(self, \"_detect_hook_handle\") and self._detect_hook_handle is not None:\n",
        "            self._detect_hook_handle.remove()\n",
        "\n",
        "        def pre_hook(module, inputs):\n",
        "            feats = list(inputs[0])  # list of multiscale neck features\n",
        "\n",
        "            if self.use_cbam:\n",
        "                # one CBAM per scale\n",
        "                if self.cbam_neck is None:\n",
        "                    self.cbam_neck = nn.ModuleList([CBAM(f.shape[1]).to(f.device) for f in feats])\n",
        "                # apply to ALL scales\n",
        "                feats = [m(f) for m, f in zip(self.cbam_neck, feats)]\n",
        "\n",
        "            self._neck_feats = feats\n",
        "            return (feats,) if self.use_cbam else None\n",
        "\n",
        "        self._detect_hook_handle = detect_module.register_forward_pre_hook(pre_hook)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_high_res_from_detect_inputs(feats):\n",
        "        if not isinstance(feats, (list, tuple)) or len(feats) == 0:\n",
        "            raise RuntimeError(\"Detect input features not captured.\")\n",
        "        return max(feats, key=lambda t: t.shape[-2] * t.shape[-1])\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_low_res_from_detect_inputs(feats):\n",
        "        if not isinstance(feats, (list, tuple)) or len(feats) == 0:\n",
        "            raise RuntimeError(\"Detect input features not captured.\")\n",
        "        return min(feats, key=lambda t: t.shape[-2] * t.shape[-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TRAIN: x is a batch dict -> YOLO returns (det_loss, loss_items)\n",
        "        if isinstance(x, dict):\n",
        "            self._neck_feats = None\n",
        "            imgs = x[\"img\"]\n",
        "            det_loss, det_items = self.yolo(x)\n",
        "\n",
        "            feat_seg = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "            seg_logits = self.sem_head(feat_seg)\n",
        "            seg_logits = F.interpolate(seg_logits, size=imgs.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "            feat_dn = self._pick_low_res_from_detect_inputs(self._neck_feats)\n",
        "            dn_logits = self.dn_head(feat_dn)\n",
        "\n",
        "            return det_loss, det_items, seg_logits, dn_logits\n",
        "\n",
        "        # INFER: x is an image tensor -> YOLO returns preds\n",
        "        self._neck_feats = None\n",
        "        det_preds = self.yolo(x)\n",
        "\n",
        "        feat_seg = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "        seg_logits = self.sem_head(feat_seg)\n",
        "        seg_logits = F.interpolate(seg_logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        feat_dn = self._pick_low_res_from_detect_inputs(self._neck_feats)\n",
        "        dn_logits = self.dn_head(feat_dn)\n",
        "\n",
        "        return det_preds, seg_logits, dn_logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whth5Do2YwDB",
        "outputId": "67942913-c463-4ad5-aa9a-5384236ccf16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_det_seg(batch):\n",
        "    # batch items: (img, labels, mask, dn)\n",
        "    imgs, labels_list, masks, dns = zip(*batch)\n",
        "\n",
        "    imgs = torch.stack(imgs, 0)         # [B,3,H,W]\n",
        "    masks = torch.stack(masks, 0)       # [B,1,H,W]\n",
        "    dn = torch.tensor(dns, dtype=torch.long)  # [B]\n",
        "\n",
        "    bboxes_all, cls_all, batch_idx_all = [], [], []\n",
        "    for i, lab in enumerate(labels_list):\n",
        "        if lab.numel() == 0:\n",
        "            continue\n",
        "        cls = lab[:, 0:1].long()\n",
        "        bboxes = lab[:, 1:5].float()\n",
        "        bboxes_all.append(bboxes)\n",
        "        cls_all.append(cls)\n",
        "        batch_idx_all.append(torch.full((lab.shape[0],), i, dtype=torch.long))\n",
        "\n",
        "    if len(bboxes_all):\n",
        "        bboxes = torch.cat(bboxes_all, 0)\n",
        "        cls = torch.cat(cls_all, 0)\n",
        "        batch_idx = torch.cat(batch_idx_all, 0)\n",
        "    else:\n",
        "        bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "        cls = torch.zeros((0, 1), dtype=torch.long)\n",
        "        batch_idx = torch.zeros((0,), dtype=torch.long)\n",
        "\n",
        "    # IMPORTANT: YOLO batch dict must contain ONLY what YOLO needs\n",
        "    yolo_batch = {\"img\": imgs, \"bboxes\": bboxes, \"cls\": cls, \"batch_idx\": batch_idx}\n",
        "\n",
        "    # return extras separately (so YOLO loss doesn't see them)\n",
        "    return yolo_batch, masks, dn\n",
        "\n"
      ],
      "metadata": {
        "id": "DFL8LsoiY2nH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_iou(model, loader, device, max_batches=None):\n",
        "    model.eval()\n",
        "    total_iou = 0.0\n",
        "    total_imgs = 0\n",
        "\n",
        "    for bi, (det_batch, mask, dn) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bi >= max_batches):\n",
        "            break\n",
        "\n",
        "        det_batch = {k: v.to(device, non_blocking=True) for k, v in det_batch.items()}\n",
        "        gt = (mask.to(device, non_blocking=True) > 0.5).float()\n",
        "\n",
        "        det_loss, det_items, seg_logits, dn_logits = model(det_batch)  # dict-path returns 4\n",
        "        pred = (torch.sigmoid(seg_logits) > 0.5).float()\n",
        "\n",
        "        inter = (pred * gt).sum(dim=(1, 2, 3))\n",
        "        union = ((pred + gt) > 0).float().sum(dim=(1, 2, 3)).clamp_min(1.0)\n",
        "\n",
        "        iou = inter / union\n",
        "        total_iou += iou.sum().item()\n",
        "        total_imgs += iou.numel()\n",
        "\n",
        "    return total_iou / max(1, total_imgs)\n"
      ],
      "metadata": {
        "id": "_V6zDxiTY7D3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = BDDDetDrivableDataset(\n",
        "    yolo_root=\"/content/BDD100K_640/yolo_640\",\n",
        "    mask_root=\"/content/BDD100K_640/drivable_masks_640\",\n",
        "    split=\"test\",\n",
        "    imgsz=640,\n",
        "    dn_csv_path=\"/content/daynight_labels.csv\"\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        "    persistent_workers=False,\n",
        "    collate_fn=collate_det_seg\n",
        ")\n"
      ],
      "metadata": {
        "id": "jJlRiiVyY_eS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FINAL TRI-TASK EVALUATION\n",
        "#  - Detection: Ultralytics .val() on val + test\n",
        "#  - Segmentation: mean IoU on val + test (binary)\n",
        "#  - Day/Night: acc + balanced acc + F1 on val + test\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.models.yolo.detect.train import DetectionTrainer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dev_override = 0 if device == \"cuda\" else \"cpu\"\n",
        "\n",
        "# PATHS\n",
        "data_yaml = \"/content/BDD100K_640/yolo_640/dataset_640.yaml\"\n",
        "\n",
        "# tri task cbam weights\n",
        "best_wrapper_pt = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt\"\n",
        "\n",
        "# a YOLO checkpoint that Ultralytics can load\n",
        "det_base_pt = \"/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt\"\n",
        "\n",
        "imgsz = 640\n",
        "batch = 8\n",
        "\n",
        "\n",
        "# 1) Building the model\n",
        "\n",
        "model = YOLOv8DetSemSeg(yolo_weights=det_base_pt, use_cbam=True).to(device)\n",
        "\n",
        "trainer = DetectionTrainer(overrides={\n",
        "    \"model\": det_base_pt,\n",
        "    \"data\":  data_yaml,\n",
        "    \"imgsz\": imgsz,\n",
        "    \"device\": dev_override,\n",
        "    \"batch\": batch,\n",
        "})\n",
        "trainer.setup_model()\n",
        "trainer.model.args = trainer.args\n",
        "trainer.model.init_criterion()\n",
        "\n",
        "# swap YOLO model into wrapper\n",
        "model.yolo = trainer.model.to(device)\n",
        "\n",
        "# re-hook\n",
        "model._neck_feats = None\n",
        "model.cbam_backbone = None\n",
        "model.cbam_neck = None\n",
        "model._register_backbone_hook_point1()\n",
        "model._register_detect_input_hook()\n",
        "\n",
        "# load wrapper state_dict\n",
        "sd = torch.load(best_wrapper_pt, map_location=device)\n",
        "if isinstance(sd, dict) and \"state_dict\" in sd:\n",
        "    sd = sd[\"state_dict\"]\n",
        "model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "\n",
        "print(\"OK: loaded wrapper best:\", best_wrapper_pt)\n",
        "\n",
        "\n",
        "# 2) DETECTION EVAL\n",
        "\n",
        "yolo = YOLO(det_base_pt)\n",
        "yolo.model = model.yolo  # use the swapped, trained model\n",
        "yolo.model.names = {0:\"car\", 1:\"truck\", 2:\"bus\", 3:\"person\", 4:\"bike\"}\n",
        "\n",
        "\n",
        "print(\"\\n=== DETECTION: TEST ===\")\n",
        "try:\n",
        "    det_test = yolo.val(data=data_yaml, imgsz=imgsz, device=dev_override, split=\"test\", batch=batch)\n",
        "except Exception as e:\n",
        "    det_test = None\n",
        "    print(\"No test split or error running test val():\", repr(e))\n",
        "\n",
        "\n",
        "# 3) SEG + DAY/NIGHT eval\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_seg_dn(model, loader, device, max_batches=None):\n",
        "    model.eval()\n",
        "\n",
        "    # seg IoU\n",
        "    inter_sum = 0.0\n",
        "    union_sum = 0.0\n",
        "\n",
        "    # dn metrics (ignore dn=-1)\n",
        "    tp = tn = fp = fn = 0\n",
        "\n",
        "    n_batches = 0\n",
        "    for bi, (det_batch, mask, dn) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bi >= max_batches):\n",
        "            break\n",
        "\n",
        "        det_batch = {k: v.to(device, non_blocking=True) for k, v in det_batch.items()}\n",
        "        gt = (mask.to(device, non_blocking=True) > 0.5).float()\n",
        "        dn = dn.to(device, non_blocking=True).long()\n",
        "        _, _, seg_logits, dn_logits = model(det_batch)\n",
        "\n",
        "\n",
        "        pred = (torch.sigmoid(seg_logits) > 0.5).float()\n",
        "\n",
        "        inter = (pred * gt).sum(dim=(1,2,3))\n",
        "        union = ((pred + gt) > 0).float().sum(dim=(1,2,3))\n",
        "        inter_sum += inter.sum().item()\n",
        "        union_sum += union.sum().item()\n",
        "\n",
        "        valid = (dn >= 0)\n",
        "        if valid.any():\n",
        "            pred_dn = torch.argmax(dn_logits[valid], dim=1)\n",
        "            true_dn = dn[valid]\n",
        "\n",
        "            # positive class = 1 (night), negative = 0 (day)\n",
        "            tp += int(((pred_dn == 1) & (true_dn == 1)).sum().item())\n",
        "            tn += int(((pred_dn == 0) & (true_dn == 0)).sum().item())\n",
        "            fp += int(((pred_dn == 1) & (true_dn == 0)).sum().item())\n",
        "            fn += int(((pred_dn == 0) & (true_dn == 1)).sum().item())\n",
        "\n",
        "        n_batches += 1\n",
        "\n",
        "    miou = (inter_sum / max(1.0, union_sum))\n",
        "\n",
        "    # dn acc/balanced acc/f1\n",
        "    total = tp + tn + fp + fn\n",
        "    acc = (tp + tn) / max(1, total)\n",
        "\n",
        "    tpr = tp / max(1, (tp + fn))  # recall night\n",
        "    tnr = tn / max(1, (tn + fp))  # recall day\n",
        "    bal_acc = 0.5 * (tpr + tnr)\n",
        "\n",
        "    prec = tp / max(1, (tp + fp))\n",
        "    rec  = tpr\n",
        "    f1 = (2 * prec * rec) / max(1e-12, (prec + rec))\n",
        "\n",
        "    cm = {\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp}\n",
        "    return miou, acc, bal_acc, f1, cm\n",
        "\n",
        "\n",
        "print(\"\\n=== SEG + DAY/NIGHT: TEST  ===\")\n",
        "try:\n",
        "    test_miou, test_acc, test_bal, test_f1, test_cm = eval_seg_dn(model, test_loader, device)\n",
        "    print(f\"SEG mean IoU: {test_miou:.4f}\")\n",
        "    print(f\"DN acc: {test_acc:.4f} | balanced acc: {test_bal:.4f} | F1(night=1): {test_f1:.4f}\")\n",
        "    print(\"DN confusion (tn,fp,fn,tp):\", test_cm)\n",
        "except NameError:\n",
        "    print(\"No test_loader defined. (If you have a test split, build test_ds/test_loader like val_loader.)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofUZhv6_Z8Bx",
        "outputId": "a5c5187f-80d9-4812-d9d3-a0e9dfedc710"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.6 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/BDD100K_640/yolo_640/dataset_640.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "OK: loaded wrapper best: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt\n",
            "\n",
            "=== DETECTION: TEST ===\n",
            "Ultralytics 8.4.6 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 73 layers, 3,006,623 parameters, 13,455 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1967.9¬±711.5 MB/s, size: 62.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/BDD100K_640/yolo_640/test/labels.cache... 20000 images, 143 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20000/20000 4.4Git/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/BDD100K_640/yolo_640/test/images/e6f10c58-c46de527.jpg: 1 duplicate labels removed\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2500/2500 12.6it/s 3:19\n",
            "                   all      20000     243649      0.562      0.424      0.428      0.238\n",
            "                   car      19776     205093      0.747      0.638      0.696      0.403\n",
            "                 truck       5500       8701      0.467      0.448      0.397      0.254\n",
            "                   bus       2459       3217      0.479      0.399      0.358      0.242\n",
            "                person       6213      24641      0.683      0.373      0.451      0.201\n",
            "                  bike       1182       1997      0.434      0.263      0.235     0.0919\n",
            "Speed: 1.0ms preprocess, 3.9ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val6\u001b[0m\n",
            "\n",
            "=== SEG + DAY/NIGHT: TEST  ===\n",
            "SEG mean IoU: 0.7029\n",
            "DN acc: 0.8957 | balanced acc: 0.8941 | F1(night=1): 0.8803\n",
            "DN confusion (tn,fp,fn,tp): {'tn': 9469, 'fp': 977, 'fn': 950, 'tp': 7086}\n"
          ]
        }
      ]
    }
  ]
}