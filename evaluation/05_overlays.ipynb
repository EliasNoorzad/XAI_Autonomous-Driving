{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPku/hfb7E5DoGUbkDVWqh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliasNoorzad/XAI_Autonomous-Driving/blob/main/evaluation/05_overlays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U8vtJw2d9c7",
        "outputId": "417fb00e-a918-44ce-cc94-ec16d599a503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install ultralytics==8.4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrdIHv49eShk",
        "outputId": "06766ced-b0fd-42dc-dba2-4caeb1554532"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/XAI_Project/BDD100K_640.zip /content/"
      ],
      "metadata": {
        "id": "hzNVPOSCeSme"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/XAI_Project/daynight_labels.csv /content/"
      ],
      "metadata": {
        "id": "WKv-XXjDeSo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/BDD100K_640.zip -d /content/BDD100K_640"
      ],
      "metadata": {
        "id": "3diKaRg9eSrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml = \"\"\"\\\n",
        "path: /content/BDD100K_640/yolo_640\n",
        "train: train/images\n",
        "val: val/images\n",
        "test: test/images\n",
        "\n",
        "nc: 5\n",
        "names: [car, truck, bus, person, bike]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/BDD100K_640/yolo_640/dataset_640.yaml\", \"w\") as f:\n",
        "    f.write(yaml)"
      ],
      "metadata": {
        "id": "ZCMfddW-eStm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class BDDDetDrivableDataset(Dataset):\n",
        "    \"\"\"\n",
        "    For preprocessed 640 dataset (yolo_640 + drivable_masks_640):\n",
        "      images: <yolo_root>/<split>/images/<stem>.jpg\n",
        "      labels: <yolo_root>/<split>/labels/<stem>.txt\n",
        "      masks : <mask_root>/<split>/<stem>.png\n",
        "\n",
        "    Returns:\n",
        "      img   : FloatTensor [3, H, W] in [0,1]\n",
        "      labels: FloatTensor [N, 5] where each row is [cls, x, y, w, h] (YOLO normalized)\n",
        "      mask  : FloatTensor [1, H, W] with values 0/1\n",
        "      dn    : LongTensor scalar (0=day, 1=night)  <-- always valid (unlabeled images are filtered out)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        yolo_root: str,\n",
        "        mask_root: str,\n",
        "        split: str,\n",
        "        imgsz: int = 640,\n",
        "        dn_csv_path: str | None = None,\n",
        "    ):\n",
        "        self.yolo_root = Path(yolo_root)\n",
        "        self.mask_root = Path(mask_root)\n",
        "        self.split = split\n",
        "        self.imgsz = int(imgsz)\n",
        "\n",
        "        self.img_dir = self.yolo_root / split / \"images\"\n",
        "        self.lbl_dir = self.yolo_root / split / \"labels\"\n",
        "        self.msk_dir = self.mask_root / split\n",
        "\n",
        "        if not self.img_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing images dir: {self.img_dir}\")\n",
        "        if not self.lbl_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing labels dir: {self.lbl_dir}\")\n",
        "        if not self.msk_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing masks dir:  {self.msk_dir}\")\n",
        "\n",
        "        exts = {\".jpg\", \".jpeg\", \".png\"}\n",
        "        self.img_paths = sorted([p for p in self.img_dir.iterdir() if p.suffix.lower() in exts])\n",
        "        if len(self.img_paths) == 0:\n",
        "            raise FileNotFoundError(f\"No images found in: {self.img_dir}\")\n",
        "\n",
        "        # day/night mapping from CSV\n",
        "        if dn_csv_path is None:\n",
        "            raise RuntimeError(\"dn_csv_path is required for this dataset (day/night head training).\")\n",
        "\n",
        "        dn_csv_path = Path(dn_csv_path)\n",
        "        if not dn_csv_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing day/night CSV: {dn_csv_path}\")\n",
        "\n",
        "        dn_map = {}\n",
        "        with open(dn_csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            required = {\"split\", \"image_id\", \"label\"}\n",
        "            if not required.issubset(set(reader.fieldnames or [])):\n",
        "                raise ValueError(f\"dn CSV must have columns {required}, got {reader.fieldnames}\")\n",
        "\n",
        "            for row in reader:\n",
        "                if row[\"split\"] != self.split:\n",
        "                    continue\n",
        "\n",
        "                image_id = row[\"image_id\"].strip()   # stem (no extension)\n",
        "                lab = row[\"label\"].strip().lower()\n",
        "\n",
        "                if lab == \"day\":\n",
        "                    dn = 0\n",
        "                elif lab == \"night\":\n",
        "                    dn = 1\n",
        "                else:\n",
        "                    # if CSV contains anything else, it's a data error\n",
        "                    raise ValueError(f\"Invalid dn label in CSV for {image_id}: {row['label']}\")\n",
        "\n",
        "                dn_map[image_id] = dn\n",
        "\n",
        "        self.dn_map = dn_map\n",
        "\n",
        "        #  FILTER OUT UNLABELED IMAGES\n",
        "        before = len(self.img_paths)\n",
        "        self.img_paths = [p for p in self.img_paths if p.stem in self.dn_map]\n",
        "        after = len(self.img_paths)\n",
        "        if after == 0:\n",
        "            raise RuntimeError(f\"No labeled (day/night) images found for split='{self.split}'.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_yolo_labels(label_path: Path) -> torch.Tensor:\n",
        "        if not label_path.exists():\n",
        "            return torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "        rows = []\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                cls, x, y, w, h = parts\n",
        "                rows.append([float(cls), float(x), float(y), float(w), float(h)])\n",
        "\n",
        "        if len(rows) == 0:\n",
        "            return torch.zeros((0, 5), dtype=torch.float32)\n",
        "        return torch.tensor(rows, dtype=torch.float32)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pil_to_chw_float(img: Image.Image) -> torch.Tensor:\n",
        "        arr = np.array(img, dtype=np.float32) / 255.0\n",
        "        arr = np.transpose(arr, (2, 0, 1))\n",
        "        return torch.from_numpy(arr)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        img_path = self.img_paths[idx]\n",
        "        stem = img_path.stem\n",
        "\n",
        "        label_path = self.lbl_dir / f\"{stem}.txt\"\n",
        "        mask_path = self.msk_dir / f\"{stem}.png\"\n",
        "\n",
        "        if not mask_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing mask for {stem}: {mask_path}\")\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        labels = self._read_yolo_labels(label_path)\n",
        "\n",
        "        if img.size != (self.imgsz, self.imgsz):\n",
        "            img = img.resize((self.imgsz, self.imgsz), resample=Image.BILINEAR)\n",
        "        if mask.size != (self.imgsz, self.imgsz):\n",
        "            mask = mask.resize((self.imgsz, self.imgsz), resample=Image.NEAREST)\n",
        "\n",
        "        img_t = self._pil_to_chw_float(img)\n",
        "        mask_np = (np.array(mask, dtype=np.uint8) > 0).astype(np.float32)\n",
        "        mask_t = torch.from_numpy(mask_np)[None, :, :]\n",
        "\n",
        "        # dn is ALWAYS valid because we filtered img_paths\n",
        "        dn = torch.tensor(self.dn_map[stem], dtype=torch.long)\n",
        "\n",
        "        return img_t, labels, mask_t, dn\n"
      ],
      "metadata": {
        "id": "crfT1jGpeSvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 16):\n",
        "        super().__init__()\n",
        "        hidden = max(channels // reduction, 1)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # shared MLP (implemented with 1x1 convs)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(channels, hidden, kernel_size=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, channels, kernel_size=1, bias=False),\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        avg_out = self.mlp(self.avg_pool(x))\n",
        "        max_out = self.mlp(self.max_pool(x))\n",
        "        w = self.sigmoid(avg_out + max_out)  # BxCx1x1\n",
        "        return x * w\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size: int = 7):\n",
        "        super().__init__()\n",
        "        assert kernel_size in (3, 7)\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.last_sa = None  # <--- add this\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        mean_map = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        m = torch.cat([mean_map, max_map], dim=1)\n",
        "\n",
        "        w = self.sigmoid(self.conv(m))  # Bx1xHxW in [0,1]\n",
        "        self.last_sa = w.detach()\n",
        "        return x * w\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 16, spatial_kernel: int = 7):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
        "        self.sa = SpatialAttention(kernel_size=spatial_kernel)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.ca(x)\n",
        "        x = self.sa(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "F0bcI015eSyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "class YOLOv8DetSemSeg(nn.Module):\n",
        "    \"\"\"\n",
        "    YOLOv8 detection model + tiny semantic seg head.\n",
        "    Captures NECK features by hooking the Detect head INPUT (multi-scale features).\n",
        "    \"\"\"\n",
        "    def __init__(self, yolo_weights: str = \"yolov8n.pt\", use_cbam: bool = False):\n",
        "        super().__init__()\n",
        "        self.yolo = YOLO(yolo_weights).model  # nn.Module\n",
        "        self.use_cbam = use_cbam\n",
        "\n",
        "        self.cbam_backbone = None  # Point 1 (after last backbone block)\n",
        "        self.cbam_neck = None      # Point 2 (once in neck before heads)\n",
        "\n",
        "        self._neck_feats = None\n",
        "\n",
        "        self.sem_head = nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self._register_backbone_hook_point1()\n",
        "        self._register_detect_input_hook()\n",
        "\n",
        "    def _register_backbone_hook_point1(self):\n",
        "        # find first upsample -> previous layer output is your last backbone high-level feature\n",
        "        idx_up = None\n",
        "        for i, m in enumerate(self.yolo.model):\n",
        "            if isinstance(m, nn.Upsample) or \"upsample\" in m.__class__.__name__.lower():\n",
        "                idx_up = i\n",
        "                break\n",
        "        if idx_up is None or idx_up == 0:\n",
        "            raise RuntimeError(\"Could not find neck start (Upsample) to place backbone CBAM.\")\n",
        "\n",
        "        backbone_last = self.yolo.model[idx_up - 1]\n",
        "\n",
        "        if hasattr(self, \"_bb_hook_handle\") and self._bb_hook_handle is not None:\n",
        "            self._bb_hook_handle.remove()\n",
        "\n",
        "        def fwd_hook(module, inputs, output):\n",
        "            if not self.use_cbam:\n",
        "                return None\n",
        "            if self.cbam_backbone is None:\n",
        "                self.cbam_backbone = CBAM(channels=output.shape[1]).to(output.device)\n",
        "            return self.cbam_backbone(output)\n",
        "\n",
        "        self._bb_hook_handle = backbone_last.register_forward_hook(fwd_hook)\n",
        "\n",
        "    def _register_detect_input_hook(self):\n",
        "        if not hasattr(self.yolo, \"model\"):\n",
        "            raise RuntimeError(\"Unexpected Ultralytics model: no .model\")\n",
        "\n",
        "        detect_module = self.yolo.model[-1]\n",
        "        name = detect_module.__class__.__name__.lower()\n",
        "        if \"detect\" not in name:\n",
        "            raise RuntimeError(f\"Last module is not Detect (got {detect_module.__class__.__name__}).\")\n",
        "\n",
        "        # remove previous hook if exists\n",
        "        if hasattr(self, \"_detect_hook_handle\") and self._detect_hook_handle is not None:\n",
        "            self._detect_hook_handle.remove()\n",
        "\n",
        "        def pre_hook(module, inputs):\n",
        "            feats = inputs[0]              # should be list of tensors\n",
        "            feats = list(feats)            # FORCE list\n",
        "\n",
        "            if self.use_cbam:\n",
        "                if self.cbam_neck is None:\n",
        "                    self.cbam_neck = CBAM(feats[-1].shape[1]).to(feats[-1].device)\n",
        "                feats[-1] = self.cbam_neck(feats[-1])\n",
        "\n",
        "            self._neck_feats = feats\n",
        "            return (feats,) if self.use_cbam else None\n",
        "          # return list inside the tuple wrapper\n",
        "\n",
        "\n",
        "        self._detect_hook_handle = detect_module.register_forward_pre_hook(pre_hook)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_high_res_from_detect_inputs(feats):\n",
        "        # feats: list/tuple of [B,C,H,W]\n",
        "        if not isinstance(feats, (list, tuple)) or len(feats) == 0:\n",
        "            raise RuntimeError(\"Detect input features not captured.\")\n",
        "        return max(feats, key=lambda t: t.shape[-2] * t.shape[-1])  # highest H*W (usually P3)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # TRAIN: x is a batch dict -> YOLO returns (det_loss, loss_items)\n",
        "      if isinstance(x, dict):\n",
        "          self._neck_feats = None\n",
        "          imgs = x[\"img\"]\n",
        "          det_loss, det_items = self.yolo(x)\n",
        "\n",
        "          feat = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "          seg_logits = self.sem_head(feat)\n",
        "          seg_logits = F.interpolate(seg_logits, size=imgs.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "          return det_loss, det_items, seg_logits\n",
        "\n",
        "      # INFER: x is an image tensor -> YOLO returns preds\n",
        "      self._neck_feats = None\n",
        "      det_preds = self.yolo(x)\n",
        "\n",
        "      feat = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "      seg_logits = self.sem_head(feat)\n",
        "      seg_logits = F.interpolate(seg_logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "      return det_preds, seg_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9knUAaVeS02",
        "outputId": "f3f1cce1-3588-41d4-96ad-3dc6873b126c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "class YOLOv8DetSemSegDn(nn.Module):\n",
        "    \"\"\"\n",
        "    YOLOv8 detection model + tiny semantic seg head.\n",
        "    Captures NECK features by hooking the Detect head INPUT (multi-scale features).\n",
        "    \"\"\"\n",
        "    def __init__(self, yolo_weights: str = \"yolov8n.pt\", use_cbam: bool = False):\n",
        "        super().__init__()\n",
        "        self.yolo = YOLO(yolo_weights).model  # nn.Module\n",
        "        self.use_cbam = use_cbam\n",
        "\n",
        "        self.cbam_backbone = None  # Point 1 (after last backbone block)\n",
        "        self.cbam_neck = None      # Point 2 (CBAM on ALL neck feature maps)\n",
        "\n",
        "        self._neck_feats = None\n",
        "\n",
        "        self.sem_head = nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.dn_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),  # [B,C,1,1]\n",
        "            nn.Flatten(1),            # [B,C]\n",
        "            nn.LazyLinear(2)          # [B,2] logits: [day, night]\n",
        "        )\n",
        "\n",
        "        self._register_backbone_hook_point1()\n",
        "        self._register_detect_input_hook()\n",
        "\n",
        "    def _register_backbone_hook_point1(self):\n",
        "        idx_up = None\n",
        "        for i, m in enumerate(self.yolo.model):\n",
        "            if isinstance(m, nn.Upsample) or \"upsample\" in m.__class__.__name__.lower():\n",
        "                idx_up = i\n",
        "                break\n",
        "        if idx_up is None or idx_up == 0:\n",
        "            raise RuntimeError(\"Could not find neck start (Upsample) to place backbone CBAM.\")\n",
        "\n",
        "        backbone_last = self.yolo.model[idx_up - 1]\n",
        "\n",
        "        if hasattr(self, \"_bb_hook_handle\") and self._bb_hook_handle is not None:\n",
        "            self._bb_hook_handle.remove()\n",
        "\n",
        "        def fwd_hook(module, inputs, output):\n",
        "            if not self.use_cbam:\n",
        "                return None\n",
        "            if self.cbam_backbone is None:\n",
        "                self.cbam_backbone = CBAM(channels=output.shape[1]).to(output.device)\n",
        "            return self.cbam_backbone(output)\n",
        "\n",
        "        self._bb_hook_handle = backbone_last.register_forward_hook(fwd_hook)\n",
        "\n",
        "    def _register_detect_input_hook(self):\n",
        "        if not hasattr(self.yolo, \"model\"):\n",
        "            raise RuntimeError(\"Unexpected Ultralytics model: no .model\")\n",
        "\n",
        "        detect_module = self.yolo.model[-1]\n",
        "        name = detect_module.__class__.__name__.lower()\n",
        "        if \"detect\" not in name:\n",
        "            raise RuntimeError(f\"Last module is not Detect (got {detect_module.__class__.__name__}).\")\n",
        "\n",
        "        if hasattr(self, \"_detect_hook_handle\") and self._detect_hook_handle is not None:\n",
        "            self._detect_hook_handle.remove()\n",
        "\n",
        "        def pre_hook(module, inputs):\n",
        "            feats = list(inputs[0])  # list of multiscale neck features\n",
        "\n",
        "            if self.use_cbam:\n",
        "                # one CBAM per scale\n",
        "                if self.cbam_neck is None:\n",
        "                    self.cbam_neck = nn.ModuleList([CBAM(f.shape[1]).to(f.device) for f in feats])\n",
        "                # apply to ALL scales\n",
        "                feats = [m(f) for m, f in zip(self.cbam_neck, feats)]\n",
        "\n",
        "            self._neck_feats = feats\n",
        "            return (feats,) if self.use_cbam else None\n",
        "\n",
        "        self._detect_hook_handle = detect_module.register_forward_pre_hook(pre_hook)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_high_res_from_detect_inputs(feats):\n",
        "        if not isinstance(feats, (list, tuple)) or len(feats) == 0:\n",
        "            raise RuntimeError(\"Detect input features not captured.\")\n",
        "        return max(feats, key=lambda t: t.shape[-2] * t.shape[-1])\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_low_res_from_detect_inputs(feats):\n",
        "        if not isinstance(feats, (list, tuple)) or len(feats) == 0:\n",
        "            raise RuntimeError(\"Detect input features not captured.\")\n",
        "        return min(feats, key=lambda t: t.shape[-2] * t.shape[-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TRAIN: x is a batch dict -> YOLO returns (det_loss, loss_items)\n",
        "        if isinstance(x, dict):\n",
        "            self._neck_feats = None\n",
        "            imgs = x[\"img\"]\n",
        "            det_loss, det_items = self.yolo(x)\n",
        "\n",
        "            feat_seg = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "            seg_logits = self.sem_head(feat_seg)\n",
        "            seg_logits = F.interpolate(seg_logits, size=imgs.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "            feat_dn = self._pick_low_res_from_detect_inputs(self._neck_feats)\n",
        "            dn_logits = self.dn_head(feat_dn)\n",
        "\n",
        "            return det_loss, det_items, seg_logits, dn_logits\n",
        "\n",
        "        # INFER: x is an image tensor -> YOLO returns preds\n",
        "        self._neck_feats = None\n",
        "        det_preds = self.yolo(x)\n",
        "\n",
        "        feat_seg = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "        seg_logits = self.sem_head(feat_seg)\n",
        "        seg_logits = F.interpolate(seg_logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        feat_dn = self._pick_low_res_from_detect_inputs(self._neck_feats)\n",
        "        dn_logits = self.dn_head(feat_dn)\n",
        "\n",
        "        return det_preds, seg_logits, dn_logits"
      ],
      "metadata": {
        "id": "dxepTJUKFO_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_det_seg(batch):\n",
        "    # batch items: (img, labels, mask, dn)\n",
        "    imgs, labels_list, masks, dns = zip(*batch)\n",
        "\n",
        "    imgs = torch.stack(imgs, 0)         # [B,3,H,W]\n",
        "    masks = torch.stack(masks, 0)       # [B,1,H,W]\n",
        "    dn = torch.tensor(dns, dtype=torch.long)  # [B]\n",
        "\n",
        "    bboxes_all, cls_all, batch_idx_all = [], [], []\n",
        "    for i, lab in enumerate(labels_list):\n",
        "        if lab.numel() == 0:\n",
        "            continue\n",
        "        cls = lab[:, 0:1].long()\n",
        "        bboxes = lab[:, 1:5].float()\n",
        "        bboxes_all.append(bboxes)\n",
        "        cls_all.append(cls)\n",
        "        batch_idx_all.append(torch.full((lab.shape[0],), i, dtype=torch.long))\n",
        "\n",
        "    if len(bboxes_all):\n",
        "        bboxes = torch.cat(bboxes_all, 0)\n",
        "        cls = torch.cat(cls_all, 0)\n",
        "        batch_idx = torch.cat(batch_idx_all, 0)\n",
        "    else:\n",
        "        bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "        cls = torch.zeros((0, 1), dtype=torch.long)\n",
        "        batch_idx = torch.zeros((0,), dtype=torch.long)\n",
        "\n",
        "    # IMPORTANT: YOLO batch dict must contain ONLY what YOLO needs\n",
        "    yolo_batch = {\"img\": imgs, \"bboxes\": bboxes, \"cls\": cls, \"batch_idx\": batch_idx}\n",
        "\n",
        "    # return extras separately (so YOLO loss doesn't see them)\n",
        "    return yolo_batch, masks, dn\n",
        "\n"
      ],
      "metadata": {
        "id": "8BfAy_UJe0BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_iou(model, loader, device, max_batches=None):\n",
        "    model.eval()\n",
        "    total_iou = 0.0\n",
        "    total_imgs = 0\n",
        "\n",
        "    for bi, (det_batch, mask, dn) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bi >= max_batches):\n",
        "            break\n",
        "\n",
        "        det_batch = {k: v.to(device, non_blocking=True) for k, v in det_batch.items()}\n",
        "        gt = (mask.to(device, non_blocking=True) > 0.5).float()\n",
        "\n",
        "        det_loss, det_items, seg_logits, dn_logits = model(det_batch)  # dict-path returns 4\n",
        "        pred = (torch.sigmoid(seg_logits) > 0.5).float()\n",
        "\n",
        "        inter = (pred * gt).sum(dim=(1, 2, 3))\n",
        "        union = ((pred + gt) > 0).float().sum(dim=(1, 2, 3)).clamp_min(1.0)\n",
        "\n",
        "        iou = inter / union\n",
        "        total_iou += iou.sum().item()\n",
        "        total_imgs += iou.numel()\n",
        "\n",
        "    return total_iou / max(1, total_imgs)\n"
      ],
      "metadata": {
        "id": "p2LrYyzie0mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = BDDDetDrivableDataset(\n",
        "    yolo_root=\"/content/BDD100K_640/yolo_640\",\n",
        "    mask_root=\"/content/BDD100K_640/drivable_masks_640\",\n",
        "    split=\"val\",\n",
        "    imgsz=640,\n",
        "    dn_csv_path=\"/content/daynight_labels.csv\"\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        "    persistent_workers=False,\n",
        "    collate_fn=collate_det_seg\n",
        ")"
      ],
      "metadata": {
        "id": "CBP-xPfleQ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CBAM HEATMAP\n",
        "\n",
        "import os, random, glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from ultralytics import YOLO\n",
        "det_yolo = YOLO(\"/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt\")\n",
        "det_yolo.fuse()\n",
        "\n",
        "det_yolo.model.names = {0:\"car\", 1:\"truck\", 2:\"bus\", 3:\"person\", 4:\"bike\"}\n",
        "\n",
        "\n",
        "# CONFIG\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "best_pt   = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_cbam_640/weights/best.pt\"\n",
        "last_ckpt = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_cbam_640/weights/last.ckpt\"\n",
        "\n",
        "out_dir = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/att_seg_dn_overlays_9\"\n",
        "N = 20\n",
        "\n",
        "alpha_att = 0.55   # max heatmap opacity (per-pixel scaled by attention)\n",
        "alpha_seg = 0.35\n",
        "\n",
        "PAD_VAL = 114\n",
        "PAD_TOL = 3\n",
        "\n",
        "# global norm quantiles\n",
        "GLOBAL_Q_LO = 0.05\n",
        "GLOBAL_Q_HI = 0.95\n",
        "\n",
        "IMG_SIZE = 640\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def overlay_attention_fit(im_rgb_u8: np.ndarray, att01: np.ndarray, alpha=0.55):\n",
        "    # att01 in [0,1], red=high (jet)\n",
        "    heat = cm.jet(att01)[..., :3]                 # [H,W,3] float [0,1]\n",
        "    heat_u8 = (heat * 255).astype(np.float32)\n",
        "\n",
        "    im = im_rgb_u8.astype(np.float32)\n",
        "    a = (alpha * att01).astype(np.float32)[..., None]   # per-pixel alpha\n",
        "    out = (1 - a) * im + a * heat_u8\n",
        "    return np.clip(out, 0, 255).astype(np.uint8)\n",
        "\n",
        "def overlay_seg_red(im_rgb_u8: np.ndarray, mask01: np.ndarray, alpha=0.35):\n",
        "    im = im_rgb_u8.astype(np.float32)\n",
        "    ov = im.copy()\n",
        "    red = np.array([255, 0, 0], dtype=np.float32)\n",
        "    m = (mask01 > 0)\n",
        "    ov[m] = (1 - alpha) * ov[m] + alpha * red\n",
        "    return np.clip(ov, 0, 255).astype(np.uint8)\n",
        "\n",
        "def get_pad_mask_rgb_u8(img_u8: np.ndarray, pad_val=114, tol=3):\n",
        "    diff = np.abs(img_u8.astype(np.int16) - pad_val)\n",
        "    return (diff[..., 0] <= tol) & (diff[..., 1] <= tol) & (diff[..., 2] <= tol)\n",
        "\n",
        "def keep_top_percent(att01: np.ndarray, nonpad_mask: np.ndarray, top_p: float = 0.15):\n",
        "    vals = att01[nonpad_mask]\n",
        "    if vals.size == 0:\n",
        "        return att01\n",
        "    thr = np.quantile(vals, 1.0 - top_p)   # keep only top top_p\n",
        "    out = att01.copy()\n",
        "    out[out < thr] = 0.0\n",
        "    return out\n",
        "\n",
        "def extract_xyxy_conf(det_out, conf_thres=0.25, max_det=50):\n",
        "    \"\"\"\n",
        "    Returns a list of boxes: [(x1,y1,x2,y2,conf), ...] in 640x640 coords.\n",
        "    Works for common formats:\n",
        "      - Tensor/ndarray Nx6: [x1,y1,x2,y2,conf,cls]\n",
        "      - Tensor Nx5: [x1,y1,x2,y2,conf]\n",
        "      - Ultralytics Results-like: det_out.boxes.xyxy + det_out.boxes.conf\n",
        "    \"\"\"\n",
        "    # unwrap list/tuple\n",
        "    if isinstance(det_out, (list, tuple)) and len(det_out) > 0:\n",
        "        det_out = det_out[0]\n",
        "\n",
        "    # Ultralytics Results-like\n",
        "    if hasattr(det_out, \"boxes\") and hasattr(det_out.boxes, \"xyxy\"):\n",
        "        xyxy = det_out.boxes.xyxy.detach().cpu().numpy()\n",
        "        conf = det_out.boxes.conf.detach().cpu().numpy()\n",
        "        out = []\n",
        "        for b, c in zip(xyxy, conf):\n",
        "            if c >= conf_thres:\n",
        "                out.append((float(b[0]), float(b[1]), float(b[2]), float(b[3]), float(c)))\n",
        "        out.sort(key=lambda t: t[4], reverse=True)\n",
        "        return out[:max_det]\n",
        "\n",
        "    # tensor/ndarray\n",
        "    if torch.is_tensor(det_out):\n",
        "        arr = det_out.detach().cpu().numpy()\n",
        "    else:\n",
        "        arr = np.array(det_out)\n",
        "\n",
        "    if arr.ndim != 2 or arr.shape[0] == 0:\n",
        "        return []\n",
        "\n",
        "    if arr.shape[1] >= 6:\n",
        "        arr = arr[:, :5]  # keep xyxy + conf\n",
        "    if arr.shape[1] == 5:\n",
        "        out = []\n",
        "        for x1, y1, x2, y2, c in arr:\n",
        "            if c >= conf_thres:\n",
        "                out.append((float(x1), float(y1), float(x2), float(y2), float(c)))\n",
        "        out.sort(key=lambda t: t[4], reverse=True)\n",
        "        return out[:max_det]\n",
        "\n",
        "    return []\n",
        "\n",
        "def attention_in_boxes(att01, boxes, H, W):\n",
        "    \"\"\"Keep attention only inside boxes, everything else 0.\n",
        "       boxes can be (x1,y1,x2,y2,conf) OR (x1,y1,x2,y2,conf,cls)\n",
        "    \"\"\"\n",
        "    m = np.zeros((H, W), dtype=np.float32)\n",
        "    for b in boxes:\n",
        "        x1, y1, x2, y2, conf = b[:5]   # take first 5, ignore cls if present\n",
        "\n",
        "        x1 = int(max(0, min(W-1, round(x1))))\n",
        "        y1 = int(max(0, min(H-1, round(y1))))\n",
        "        x2 = int(max(0, min(W,   round(x2))))\n",
        "        y2 = int(max(0, min(H,   round(y2))))\n",
        "\n",
        "        if x2 > x1 and y2 > y1:\n",
        "            m[y1:y2, x1:x2] = 1.0\n",
        "\n",
        "    return att01 * m\n",
        "\n",
        "def draw_boxes(img_u8: np.ndarray, boxes, color=(0, 255, 0), thickness=2):\n",
        "    out = img_u8.copy()\n",
        "    H, W = out.shape[:2]\n",
        "    for (x1, y1, x2, y2, conf) in boxes:\n",
        "        x1 = int(max(0, min(W-1, round(x1))))\n",
        "        y1 = int(max(0, min(H-1, round(y1))))\n",
        "        x2 = int(max(0, min(W-1, round(x2))))\n",
        "        y2 = int(max(0, min(H-1, round(y2))))\n",
        "\n",
        "        # top/bottom\n",
        "        out[max(0,y1-thickness):y1+thickness, x1:x2] = color\n",
        "        out[max(0,y2-thickness):y2+thickness, x1:x2] = color\n",
        "        # left/right\n",
        "        out[y1:y2, max(0,x1-thickness):x1+thickness] = color\n",
        "        out[y1:y2, max(0,x2-thickness):x2+thickness] = color\n",
        "    return out\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def draw_boxes_with_text(img_u8: np.ndarray, boxes, cls_names=None, color=(0,255,0), thickness=2):\n",
        "    \"\"\"\n",
        "    boxes: list of tuples either (x1,y1,x2,y2,conf) or (x1,y1,x2,y2,conf,cls)\n",
        "    \"\"\"\n",
        "    im = Image.fromarray(img_u8)\n",
        "    draw = ImageDraw.Draw(im)\n",
        "\n",
        "    # try default font\n",
        "    try:\n",
        "        font = ImageFont.load_default()\n",
        "    except:\n",
        "        font = None\n",
        "\n",
        "    for b in boxes:\n",
        "        if len(b) == 5:\n",
        "            x1,y1,x2,y2,conf = b\n",
        "            cls = None\n",
        "        else:\n",
        "            x1,y1,x2,y2,conf,cls = b\n",
        "\n",
        "        x1,y1,x2,y2 = map(int, [x1,y1,x2,y2])\n",
        "\n",
        "        # rectangle\n",
        "        for t in range(thickness):\n",
        "            draw.rectangle([x1-t, y1-t, x2+t, y2+t], outline=color)\n",
        "\n",
        "        # text\n",
        "        label = f\"{conf:.2f}\"\n",
        "        if cls is not None and cls_names is not None:\n",
        "            if isinstance(cls_names, dict):\n",
        "                name = cls_names.get(int(cls), str(int(cls)))\n",
        "            else:\n",
        "                name = cls_names[int(cls)] if int(cls) < len(cls_names) else str(int(cls))\n",
        "            label = f\"{name} {conf:.2f}\"\n",
        "\n",
        "        tx, ty = x1, max(0, y1 - 12)\n",
        "        draw.text((tx, ty), label, fill=color, font=font)\n",
        "\n",
        "    return np.array(im)\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def overlay_attention_full(im_rgb_u8, att01, nonpad_mask, alpha=0.55, cmap_name=\"plasma\"):\n",
        "    cmap = cm.get_cmap(cmap_name)\n",
        "    heat = cmap(att01)[..., :3]\n",
        "    heat_u8 = (heat * 255).astype(np.float32)\n",
        "\n",
        "    im = im_rgb_u8.astype(np.float32)\n",
        "    out = im.copy()\n",
        "\n",
        "    m = nonpad_mask\n",
        "    out[m] = (1 - alpha) * im[m] + alpha * heat_u8[m]\n",
        "    return np.clip(out, 0, 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "# 1) Build model + load weights\n",
        "det_base = \"/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt\"\n",
        "model = YOLOv8DetSemSeg(yolo_weights=det_base, use_cbam=True).to(device)\n",
        "\n",
        "# force CBAM modules to exist (if lazily created)\n",
        "with torch.no_grad():\n",
        "    _ = model(torch.randn(1, 3, IMG_SIZE, IMG_SIZE, device=device))\n",
        "\n",
        "# load weights\n",
        "if os.path.exists(best_pt):\n",
        "    sd = torch.load(best_pt, map_location=\"cpu\")\n",
        "elif os.path.exists(last_ckpt):\n",
        "    ck = torch.load(last_ckpt, map_location=\"cpu\")\n",
        "    sd = ck[\"model\"]\n",
        "else:\n",
        "    raise FileNotFoundError(\"Neither best.pt nor last.ckpt found.\")\n",
        "\n",
        "model.load_state_dict(sd, strict=True)\n",
        "model.eval()\n",
        "\n",
        "tag_model = YOLOv8DetSemSegDn(yolo_weights=det_base, use_cbam=True).to(device)   # <- your tri-task class name\n",
        "with torch.no_grad():\n",
        "    _ = tag_model(torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device))\n",
        "tag_sd = torch.load(\"/content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt\", map_location=\"cpu\")\n",
        "tag_model.load_state_dict(tag_sd, strict=True)\n",
        "tag_model.eval()\n",
        "\n",
        "tag_cache = {}\n",
        "\n",
        "\n",
        "# 2) Hook: store ALL spatial logits calls\n",
        "cb = model.cbam_neck if getattr(model, \"cbam_neck\", None) is not None else getattr(model, \"cbam_backbone\", None)\n",
        "if cb is None or (not hasattr(cb, \"sa\")) or (not hasattr(cb.sa, \"conv\")):\n",
        "    raise RuntimeError(\"CBAM.sa.conv not found.\")\n",
        "\n",
        "_att = {\"logits_list\": []}\n",
        "\n",
        "def _hook_sa_logits(mod, inp, out):\n",
        "    _att[\"logits_list\"].append(out.detach())  # [B,1,h,w] pre-sigmoid\n",
        "\n",
        "if hasattr(cb.sa, \"_sa_hook_handle\") and cb.sa._sa_hook_handle is not None:\n",
        "    cb.sa._sa_hook_handle.remove()\n",
        "cb.sa._sa_hook_handle = cb.sa.conv.register_forward_hook(_hook_sa_logits)\n",
        "\n",
        "#3) Pick random samples\n",
        "idxs = random.sample(range(len(val_ds)), k=min(N, len(val_ds)))\n",
        "\n",
        "# 4) PASS 1: compute fused attention + collect values for GLOBAL norm\n",
        "att_cache = {}   # idx -> att_map (H,W) float32 (already pad-masked, not normalized)\n",
        "seg_cache = {}   # idx -> pred_mask uint8\n",
        "img_cache = {}   # idx -> img_rgb uint8\n",
        "pad_cache = {}   # idx -> pad_mask bool\n",
        "det_cache = {}   # idx -> list of boxes\n",
        "\n",
        "\n",
        "vals_all = []    # collect attention values (non-pad) for global quantiles\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx in idxs:\n",
        "        img_t = val_ds[idx][0]# [3,640,640] in [0,1]\n",
        "        img_np = (img_t.permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
        "        H, W = img_np.shape[:2]\n",
        "\n",
        "        # img_np is 640x640 uint8 RGB already\n",
        "        res = det_yolo.predict(source=img_np, imgsz=640, conf=0.50, iou=0.6, verbose=False)[0]\n",
        "        xyxy = res.boxes.xyxy.cpu().numpy()\n",
        "        conf = res.boxes.conf.cpu().numpy()\n",
        "        cls  = res.boxes.cls.cpu().numpy()\n",
        "\n",
        "        boxes = [(float(b[0]), float(b[1]), float(b[2]), float(b[3]), float(c), int(k))\n",
        "                for b, c, k in zip(xyxy, conf, cls)]\n",
        "        det_cache[idx] = boxes\n",
        "        print(\"idx\", idx, \"boxes:\", len(boxes))\n",
        "\n",
        "\n",
        "        pad_mask = get_pad_mask_rgb_u8(img_np, PAD_VAL, PAD_TOL)\n",
        "        nonpad = ~pad_mask\n",
        "        if nonpad.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        x = img_t.unsqueeze(0).to(device)\n",
        "\n",
        "        _, _, tag_logits = tag_model(x)          # tag_logits shape [B,1] or [B]\n",
        "        cls = torch.softmax(tag_logits, dim=1).argmax(dim=1).item()   # 0 or 1\n",
        "        tag_cache[idx] = \"night\" if cls == 1 else \"day\"               # use class order\n",
        "\n",
        "\n",
        "\n",
        "        _att[\"logits_list\"].clear()\n",
        "        _det_preds, seg_logits = model(x)\n",
        "\n",
        "\n",
        "        det_cache[idx] = boxes\n",
        "\n",
        "\n",
        "        pred_mask = (torch.sigmoid(seg_logits)[0, 0] > 0.5).detach().cpu().numpy().astype(np.uint8)\n",
        "\n",
        "        if len(_att[\"logits_list\"]) == 0:\n",
        "            continue\n",
        "\n",
        "        # For each logits_i: sigmoid -> upsample to (H,W), then weighted average.\n",
        "        fused = torch.zeros((H, W), device=device, dtype=torch.float32)\n",
        "        wsum = 0.0\n",
        "\n",
        "        for logits in _att[\"logits_list\"]:\n",
        "            # logits: [B,1,h,w]\n",
        "            sa = torch.sigmoid(logits)[0, 0]  # [h,w]\n",
        "            h, w = int(sa.shape[-2]), int(sa.shape[-1])\n",
        "\n",
        "            sa_up = F.interpolate(sa[None, None], size=(H, W), mode=\"bilinear\", align_corners=False)[0, 0]\n",
        "\n",
        "            # weight finer maps more (proportional to resolution)\n",
        "            weight = float(h * w)\n",
        "            fused += weight * sa_up\n",
        "            wsum += weight\n",
        "\n",
        "        fused = fused / (wsum + 1e-6)\n",
        "\n",
        "        # pad masking (keep padding at 0 so it doesn't affect global stats)\n",
        "        fused = fused.masked_fill(torch.from_numpy(pad_mask).to(device), 0.0)\n",
        "\n",
        "        fused_np = fused.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # collect non-pad values for global quantiles\n",
        "        vals_all.append(fused_np[nonpad])\n",
        "\n",
        "        att_cache[idx] = fused_np\n",
        "        seg_cache[idx] = pred_mask\n",
        "        img_cache[idx] = img_np\n",
        "        pad_cache[idx] = pad_mask\n",
        "\n",
        "# build global quantiles\n",
        "if len(vals_all) == 0:\n",
        "    cb.sa._sa_hook_handle.remove()\n",
        "    raise RuntimeError(\"No attention values collected (hook not firing or dataset issue).\")\n",
        "\n",
        "vals_concat = np.concatenate(vals_all, axis=0)\n",
        "g_lo = float(np.quantile(vals_concat, GLOBAL_Q_LO))\n",
        "g_hi = float(np.quantile(vals_concat, GLOBAL_Q_HI))\n",
        "print(\"Global lo/hi:\", g_lo, g_hi)\n",
        "\n",
        "#5) PASS 2: global normalize + save overlays\n",
        "saved = 0\n",
        "for idx in idxs:\n",
        "    if idx not in att_cache:\n",
        "        continue\n",
        "\n",
        "    img_np    = img_cache[idx]\n",
        "    pred_mask = seg_cache[idx]\n",
        "    pad_mask  = pad_cache[idx]\n",
        "    att       = att_cache[idx]  # float32, pad already 0\n",
        "\n",
        "    H, W = img_np.shape[:2]\n",
        "    boxes = det_cache.get(idx, [])\n",
        "\n",
        "    # 1) GLOBAL NORMALIZATION\n",
        "    att01 = np.clip((att - g_lo) / (g_hi - g_lo + 1e-6), 0.0, 1.0).astype(np.float32)\n",
        "    att01[pad_mask] = 0.0\n",
        "\n",
        "    att_full = att01  # raw fused attention (normalized), no boxing, no thresholding\n",
        "    nonpad_mask = ~pad_mask\n",
        "\n",
        "    # ONE heatmap overlay (only on non-pad so bars stay gray)\n",
        "    over = overlay_attention_full(img_np, att_full, nonpad_mask, alpha=alpha_att, cmap_name=\"plasma\")\n",
        "\n",
        "    # add seg + boxes for context (doesn't change attention)\n",
        "    over = overlay_seg_red(over, pred_mask, alpha=alpha_seg)\n",
        "    over = draw_boxes_with_text(over, boxes, cls_names=det_yolo.model.names, color=(0,255,0), thickness=2)\n",
        "\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"val_{idx:05d}_attFUSED_globalNORM_seg.png\")\n",
        "    Image.fromarray(img_np).save(os.path.join(out_dir, f\"val_{idx:05d}_BASE.png\"))\n",
        "\n",
        "\n",
        "    label = tag_cache.get(idx, \"unknown\")\n",
        "    im = Image.fromarray(over)\n",
        "    draw = ImageDraw.Draw(im)\n",
        "    y0 = int(np.argmax(nonpad_mask.any(axis=1)))\n",
        "    draw.text((10, y0 + 10), f\"pred: {label}\", fill=(255,255,255))\n",
        "\n",
        "\n",
        "    im.save(out_path)\n",
        "\n",
        "    saved += 1\n",
        "\n",
        "# cleanup hook\n",
        "cb.sa._sa_hook_handle.remove()\n",
        "\n",
        "\n",
        "print(\"Saved:\", saved, \"images to:\", out_dir)\n",
        "print(\"Files now:\", len(glob.glob(out_dir + '/*.png')))\n",
        "print(\"Note: fused all CBAM.sa.conv calls + global norm quantiles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBtGbPfZZuW2",
        "outputId": "12c7fccc-8dd2-4686-9bf4-363cb323d8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary (fused): 73 layers, 3,006,623 parameters, 0 gradients, 8.1 GFLOPs\n",
            "idx 3446 boxes: 5\n",
            "idx 8132 boxes: 3\n",
            "idx 7335 boxes: 2\n",
            "idx 7767 boxes: 7\n",
            "idx 7838 boxes: 6\n",
            "idx 5347 boxes: 5\n",
            "idx 1196 boxes: 3\n",
            "idx 3172 boxes: 6\n",
            "idx 6982 boxes: 3\n",
            "idx 3685 boxes: 11\n",
            "idx 5263 boxes: 8\n",
            "idx 1557 boxes: 4\n",
            "idx 2435 boxes: 4\n",
            "idx 3294 boxes: 5\n",
            "idx 4920 boxes: 6\n",
            "idx 7470 boxes: 10\n",
            "idx 7562 boxes: 7\n",
            "idx 5178 boxes: 4\n",
            "idx 4310 boxes: 2\n",
            "idx 6574 boxes: 13\n",
            "Global lo/hi: 0.5769175291061401 0.7334437966346741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1030275437.py:206: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  cmap = cm.get_cmap(cmap_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: 20 images to: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/att_seg_dn_overlays_9\n",
            "Files now: 40\n",
            "Note: fused all CBAM.sa.conv calls + global norm quantiles.\n"
          ]
        }
      ]
    }
  ]
}