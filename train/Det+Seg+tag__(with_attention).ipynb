{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliasNoorzad/XAI_Autonomous-Driving/blob/main/train/Det%2BSeg%2Btag__(with_attention).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and use Ultralytics YOLO version 8.4.6 (used throughout the project)"
      ],
      "metadata": {
        "id": "ySjZ8BikCG1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqmNmQbqsrOL",
        "outputId": "1b3a9194-6026-4961-b6f4-7546f205285e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install ultralytics==8.4.6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive in Colab"
      ],
      "metadata": {
        "id": "Ktj0cdcXCKTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KMdppptzCn_",
        "outputId": "170c361c-9287-4c01-d314-fa2781cec5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the dataset zip from Drive to the Colab workspace"
      ],
      "metadata": {
        "id": "O7H4WQGXC0Pt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3auNmbCjzGcZ"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/XAI_Project/BDD100K_640.zip /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the day/night label file from Drive to the Colab workspace."
      ],
      "metadata": {
        "id": "qDBIpm-xC-vz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1tCAtEhGe-t"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/XAI_Project/daynight_labels.csv /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the dataset into /content/BDD100K_640"
      ],
      "metadata": {
        "id": "AUkhp0JzC4UV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyuzDKDszHut"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/BDD100K_640.zip -d /content/BDD100K_640"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and save the YOLO dataset config file (dataset.yaml)."
      ],
      "metadata": {
        "id": "C6JcKOhEDHmY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02i0EcsIzH-e"
      },
      "outputs": [],
      "source": [
        "yaml = \"\"\"\\\n",
        "path: /content/BDD100K_640/yolo_640\n",
        "train: train/images\n",
        "val: val/images\n",
        "test: test/images\n",
        "\n",
        "nc: 5\n",
        "names: [car, truck, bus, person, bike]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/BDD100K_640/yolo_640/dataset_640.yaml\", \"w\") as f:\n",
        "    f.write(yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend the dataset to support the tri-task setting by loading day/night labels from a CSV, filtering out unlabeled images, and returning each sample as (image, YOLO labels, drivable mask, day/night class)."
      ],
      "metadata": {
        "id": "a1CJ_DviEYsX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcqvwkONzIA6"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class BDDDetDrivableDataset(Dataset):\n",
        "    \"\"\"\n",
        "    For preprocessed 640 dataset (yolo_640 + drivable_masks_640):\n",
        "      images: <yolo_root>/<split>/images/<stem>.jpg\n",
        "      labels: <yolo_root>/<split>/labels/<stem>.txt\n",
        "      masks : <mask_root>/<split>/<stem>.png\n",
        "\n",
        "    Returns:\n",
        "      img   : FloatTensor [3, H, W] in [0,1]\n",
        "      labels: FloatTensor [N, 5] where each row is [cls, x, y, w, h] (YOLO normalized)\n",
        "      mask  : FloatTensor [1, H, W] with values 0/1\n",
        "      dn    : LongTensor scalar (0=day, 1=night) - unlabeled images are filtered out\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        yolo_root: str,\n",
        "        mask_root: str,\n",
        "        split: str,\n",
        "        imgsz: int = 640,\n",
        "        dn_csv_path: str | None = None,\n",
        "    ):\n",
        "        self.yolo_root = Path(yolo_root)\n",
        "        self.mask_root = Path(mask_root)\n",
        "        self.split = split\n",
        "        self.imgsz = int(imgsz)\n",
        "\n",
        "        self.img_dir = self.yolo_root / split / \"images\"\n",
        "        self.lbl_dir = self.yolo_root / split / \"labels\"\n",
        "        self.msk_dir = self.mask_root / split\n",
        "\n",
        "        if not self.img_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing images dir: {self.img_dir}\")\n",
        "        if not self.lbl_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing labels dir: {self.lbl_dir}\")\n",
        "        if not self.msk_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing masks dir:  {self.msk_dir}\")\n",
        "\n",
        "        exts = {\".jpg\", \".jpeg\", \".png\"}\n",
        "        self.img_paths = sorted([p for p in self.img_dir.iterdir() if p.suffix.lower() in exts])\n",
        "        if len(self.img_paths) == 0:\n",
        "            raise FileNotFoundError(f\"No images found in: {self.img_dir}\")\n",
        "\n",
        "        # day/night mapping from CSV\n",
        "        if dn_csv_path is None:\n",
        "            raise RuntimeError(\"dn_csv_path is required for this dataset (day/night head training).\")\n",
        "\n",
        "        dn_csv_path = Path(dn_csv_path)\n",
        "        if not dn_csv_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing day/night CSV: {dn_csv_path}\")\n",
        "\n",
        "        dn_map = {}\n",
        "        with open(dn_csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            required = {\"split\", \"image_id\", \"label\"}\n",
        "            if not required.issubset(set(reader.fieldnames or [])):\n",
        "                raise ValueError(f\"dn CSV must have columns {required}, got {reader.fieldnames}\")\n",
        "\n",
        "            for row in reader:\n",
        "                if row[\"split\"] != self.split:\n",
        "                    continue\n",
        "\n",
        "                image_id = row[\"image_id\"].strip()   # stem (no extension)\n",
        "                lab = row[\"label\"].strip().lower()\n",
        "\n",
        "                if lab == \"day\":\n",
        "                    dn = 0\n",
        "                elif lab == \"night\":\n",
        "                    dn = 1\n",
        "                else:\n",
        "                    # if CSV contains anything else, it's a data error\n",
        "                    raise ValueError(f\"Invalid dn label in CSV for {image_id}: {row['label']}\")\n",
        "\n",
        "                dn_map[image_id] = dn\n",
        "\n",
        "        self.dn_map = dn_map\n",
        "\n",
        "        # filtering out unlabeled images\n",
        "        before = len(self.img_paths)\n",
        "        self.img_paths = [p for p in self.img_paths if p.stem in self.dn_map]\n",
        "        after = len(self.img_paths)\n",
        "        if after == 0:\n",
        "            raise RuntimeError(f\"No labeled (day/night) images found for split='{self.split}'.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_yolo_labels(label_path: Path) -> torch.Tensor:\n",
        "        if not label_path.exists():\n",
        "            return torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "        rows = []\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                cls, x, y, w, h = parts\n",
        "                rows.append([float(cls), float(x), float(y), float(w), float(h)])\n",
        "\n",
        "        if len(rows) == 0:\n",
        "            return torch.zeros((0, 5), dtype=torch.float32)\n",
        "        return torch.tensor(rows, dtype=torch.float32)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pil_to_chw_float(img: Image.Image) -> torch.Tensor:\n",
        "        arr = np.array(img, dtype=np.float32) / 255.0\n",
        "        arr = np.transpose(arr, (2, 0, 1))\n",
        "        return torch.from_numpy(arr)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        img_path = self.img_paths[idx]\n",
        "        stem = img_path.stem\n",
        "\n",
        "        label_path = self.lbl_dir / f\"{stem}.txt\"\n",
        "        mask_path = self.msk_dir / f\"{stem}.png\"\n",
        "\n",
        "        if not mask_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing mask for {stem}: {mask_path}\")\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        labels = self._read_yolo_labels(label_path)\n",
        "\n",
        "        if img.size != (self.imgsz, self.imgsz):\n",
        "            img = img.resize((self.imgsz, self.imgsz), resample=Image.BILINEAR)\n",
        "        if mask.size != (self.imgsz, self.imgsz):\n",
        "            mask = mask.resize((self.imgsz, self.imgsz), resample=Image.NEAREST)\n",
        "\n",
        "        img_t = self._pil_to_chw_float(img)\n",
        "        mask_np = (np.array(mask, dtype=np.uint8) > 0).astype(np.float32)\n",
        "        mask_t = torch.from_numpy(mask_np)[None, :, :]\n",
        "\n",
        "        # dn is always valid because we filtered img_paths\n",
        "        dn = torch.tensor(self.dn_map[stem], dtype=torch.long)\n",
        "\n",
        "        return img_t, labels, mask_t, dn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity-check the tri-task dataset by loading one training sample and printing tensor shapes, value ranges, and its day/night label."
      ],
      "metadata": {
        "id": "wh1ZtW3mFKNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHIML4d2zIDS",
        "outputId": "2735dbfa-92df-4866-b722-45a7c8f69435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 640, 640]) torch.Size([2, 5]) torch.Size([1, 640, 640]) tensor(0)\n",
            "0.0 1.0 tensor([0., 1.])\n"
          ]
        }
      ],
      "source": [
        "ds = BDDDetDrivableDataset(\n",
        "    yolo_root=\"/content/BDD100K_640/yolo_640\",\n",
        "    mask_root=\"/content/BDD100K_640/drivable_masks_640\",\n",
        "    split=\"train\",\n",
        "    imgsz=640,\n",
        "    dn_csv_path=\"/content/daynight_labels.csv\"\n",
        ")\n",
        "\n",
        "img, labels, mask, dn = ds[0]\n",
        "print(img.shape, labels.shape, mask.shape, dn)\n",
        "print(img.min().item(), img.max().item(), mask.unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity-check the day/night labels: confirm the training split size (64,828 samples) and verify that labels in the first 200 samples are valid ({0,1} only)."
      ],
      "metadata": {
        "id": "5J3aA6jtFYXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"len(ds) =\", len(ds))\n",
        "\n",
        "# check first 200 samples dn values (fast)\n",
        "vals = set()\n",
        "for i in range(min(200, len(ds))):\n",
        "    _, _, _, dn_i = ds[i]\n",
        "    vals.add(int(dn_i))\n",
        "\n",
        "print(\"dn values in first batch =\", vals)\n",
        "assert vals.issubset({0, 1}), f\"Found invalid dn values: {vals}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwFeLBV76znT",
        "outputId": "52158500-be70-4a99-e1f6-331b37018f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(ds) = 64828\n",
            "dn values in first batch = {0, 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the CBAM attention block (channel attention + spatial attention) and store the spatial attention map (last_sa) for later attention overlays."
      ],
      "metadata": {
        "id": "d-ksF8y7FiUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpmXtmRtzIF9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 16):\n",
        "        super().__init__()\n",
        "        hidden = max(channels // reduction, 1)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # shared MLP (implemented with 1x1 convs)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(channels, hidden, kernel_size=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, channels, kernel_size=1, bias=False),\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        avg_out = self.mlp(self.avg_pool(x))\n",
        "        max_out = self.mlp(self.max_pool(x))\n",
        "        w = self.sigmoid(avg_out + max_out)  # BxCx1x1\n",
        "        return x * w\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size: int = 7):\n",
        "        super().__init__()\n",
        "        assert kernel_size in (3, 7)\n",
        "        padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.last_sa = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        mean_map = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        m = torch.cat([mean_map, max_map], dim=1)\n",
        "\n",
        "        w = self.sigmoid(self.conv(m))  # Bx1xHxW in [0,1]\n",
        "        self.last_sa = w.detach()\n",
        "        return x * w\n",
        "\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels: int, reduction: int = 16, spatial_kernel: int = 7):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
        "        self.sa = SpatialAttention(kernel_size=spatial_kernel)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.ca(x)\n",
        "        x = self.sa(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity-check CBAM on a random tensor to confirm the output shape matches the input and contains no NaNs ([2, 128, 80, 80])."
      ],
      "metadata": {
        "id": "yfEugcreFvDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC8fTmw0A7Sv",
        "outputId": "85053895-f234-446e-cab2-0d313360b8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBAM sanity OK: torch.Size([2, 128, 80, 80])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "x = torch.randn(2, 128, 80, 80)\n",
        "m = CBAM(channels=128, reduction=16, spatial_kernel=7)\n",
        "\n",
        "y = m(x)\n",
        "\n",
        "assert y.shape == x.shape, (x.shape, y.shape)\n",
        "assert not torch.isnan(y).any(), \"NaNs found in CBAM output\"\n",
        "print(\"CBAM sanity OK:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the tri-task YOLOv8 model (detection + drivable-area segmentation + day/night classification) with optional CBAM: apply CBAM at the backbone‚Äìneck boundary and on all neck feature scales, use the highest-resolution neck feature for segmentation logits, and the lowest-resolution feature for the day/night head."
      ],
      "metadata": {
        "id": "uBWWCBGsGJwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8zn2MDeKpL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4e3358-58e9-419b-ed73-4ced3fadc0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "class YOLOv8DetSemSeg(nn.Module):\n",
        "    \"\"\"\n",
        "    YOLOv8 detection model + tiny semantic seg head.\n",
        "    Captures NECK features by hooking the Detect head INPUT (multi-scale features).\n",
        "    \"\"\"\n",
        "    def __init__(self, yolo_weights: str = \"yolov8n.pt\", use_cbam: bool = False):\n",
        "        super().__init__()\n",
        "        self.yolo = YOLO(yolo_weights).model  # nn.Module\n",
        "        self.use_cbam = use_cbam\n",
        "\n",
        "        self.cbam_backbone = None  # Point 1 (after last backbone block)\n",
        "        self.cbam_neck = None      # Point 2 (CBAM on ALL neck feature maps)\n",
        "\n",
        "        self._neck_feats = None\n",
        "\n",
        "        self.sem_head = nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.dn_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),  # [B,C,1,1]\n",
        "            nn.Flatten(1),            # [B,C]\n",
        "            nn.LazyLinear(2)          # [B,2] logits: [day, night]\n",
        "        )\n",
        "\n",
        "        self._register_backbone_hook_point1()\n",
        "        self._register_detect_input_hook()\n",
        "\n",
        "    def _register_backbone_hook_point1(self):\n",
        "        idx_up = None\n",
        "        for i, m in enumerate(self.yolo.model):\n",
        "            if isinstance(m, nn.Upsample) or \"upsample\" in m.__class__.__name__.lower():\n",
        "                idx_up = i\n",
        "                break\n",
        "        if idx_up is None or idx_up == 0:\n",
        "            raise RuntimeError(\"Could not find neck start (Upsample) to place backbone CBAM.\")\n",
        "\n",
        "        backbone_last = self.yolo.model[idx_up - 1]\n",
        "\n",
        "        if hasattr(self, \"_bb_hook_handle\") and self._bb_hook_handle is not None:\n",
        "            self._bb_hook_handle.remove()\n",
        "\n",
        "        def fwd_hook(module, inputs, output):\n",
        "            if not self.use_cbam:\n",
        "                return None\n",
        "            if self.cbam_backbone is None:\n",
        "                self.cbam_backbone = CBAM(channels=output.shape[1]).to(output.device)\n",
        "            return self.cbam_backbone(output)\n",
        "\n",
        "        self._bb_hook_handle = backbone_last.register_forward_hook(fwd_hook)\n",
        "\n",
        "    def _register_detect_input_hook(self):\n",
        "        if not hasattr(self.yolo, \"model\"):\n",
        "            raise RuntimeError(\"Unexpected Ultralytics model: no .model\")\n",
        "\n",
        "        detect_module = self.yolo.model[-1]\n",
        "        name = detect_module.__class__.__name__.lower()\n",
        "        if \"detect\" not in name:\n",
        "            raise RuntimeError(f\"Last module is not Detect (got {detect_module.__class__.__name__}).\")\n",
        "\n",
        "        if hasattr(self, \"_detect_hook_handle\") and self._detect_hook_handle is not None:\n",
        "            self._detect_hook_handle.remove()\n",
        "\n",
        "        def pre_hook(module, inputs):\n",
        "            feats = list(inputs[0])  # list of multiscale neck features\n",
        "\n",
        "            if self.use_cbam:\n",
        "                # one CBAM per scale\n",
        "                if self.cbam_neck is None:\n",
        "                    self.cbam_neck = nn.ModuleList([CBAM(f.shape[1]).to(f.device) for f in feats])\n",
        "                # apply to ALL scales\n",
        "                feats = [m(f) for m, f in zip(self.cbam_neck, feats)]\n",
        "\n",
        "            self._neck_feats = feats\n",
        "            return (feats,) if self.use_cbam else None\n",
        "\n",
        "        self._detect_hook_handle = detect_module.register_forward_pre_hook(pre_hook)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_high_res_from_detect_inputs(feats):\n",
        "        if not isinstance(feats, (list, tuple)) or len(feats) == 0:\n",
        "            raise RuntimeError(\"Detect input features not captured.\")\n",
        "        return max(feats, key=lambda t: t.shape[-2] * t.shape[-1])\n",
        "\n",
        "    @staticmethod\n",
        "    def _pick_low_res_from_detect_inputs(feats):\n",
        "        if not isinstance(feats, (list, tuple)) or len(feats) == 0:\n",
        "            raise RuntimeError(\"Detect input features not captured.\")\n",
        "        return min(feats, key=lambda t: t.shape[-2] * t.shape[-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TRAIN: x is a batch dict -> YOLO returns (det_loss, loss_items)\n",
        "        if isinstance(x, dict):\n",
        "            self._neck_feats = None\n",
        "            imgs = x[\"img\"]\n",
        "            det_loss, det_items = self.yolo(x)\n",
        "\n",
        "            feat_seg = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "            seg_logits = self.sem_head(feat_seg)\n",
        "            seg_logits = F.interpolate(seg_logits, size=imgs.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "            feat_dn = self._pick_low_res_from_detect_inputs(self._neck_feats)\n",
        "            dn_logits = self.dn_head(feat_dn)\n",
        "\n",
        "            return det_loss, det_items, seg_logits, dn_logits\n",
        "\n",
        "        # x is an image tensor -> YOLO returns preds\n",
        "        self._neck_feats = None\n",
        "        det_preds = self.yolo(x)\n",
        "\n",
        "        feat_seg = self._pick_high_res_from_detect_inputs(self._neck_feats)\n",
        "        seg_logits = self.sem_head(feat_seg)\n",
        "        seg_logits = F.interpolate(seg_logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        feat_dn = self._pick_low_res_from_detect_inputs(self._neck_feats)\n",
        "        dn_logits = self.dn_head(feat_dn)\n",
        "\n",
        "        return det_preds, seg_logits, dn_logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity check for the tri-task CBAM model: load the best det+seg(+CBAM) checkpoint, run one labeled sample, and confirm the segmentation and day/night heads produce the expected shapes ([1,1,640,640] and [1,2]) and a valid day/night prediction for that example."
      ],
      "metadata": {
        "id": "mhtKE9YSHRUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkhN3fJ4A9uY",
        "outputId": "e35dc107-53d1-4c24-a7e9-45e1da19c1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 640, 640])\n",
            "torch.Size([1, 2])\n",
            "gt: 0 pred: 0\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# using det+seg(+CBAM) best, not det_baseline\n",
        "cbam_best = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_cbam_640/weights/best.pt\"\n",
        "\n",
        "model = YOLOv8DetSemSeg(\n",
        "    yolo_weights=cbam_best,\n",
        "    use_cbam=True\n",
        ").to(device).eval()\n",
        "\n",
        "# build Lazy + CBAM modules once\n",
        "with torch.no_grad():\n",
        "    _ = model(torch.zeros(1, 3, 640, 640, device=device))\n",
        "\n",
        "# loading weights correctly (ema/model might be Module)\n",
        "ckpt = torch.load(cbam_best, map_location=device)\n",
        "src = (ckpt.get(\"ema\") or ckpt.get(\"model\") or ckpt.get(\"state_dict\") or ckpt) if isinstance(ckpt, dict) else ckpt\n",
        "sd = src.state_dict() if isinstance(src, torch.nn.Module) else src\n",
        "model.load_state_dict(sd, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# testing one sample\n",
        "img, labels, mask, dn = ds[0]\n",
        "x = img.unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    det_out, seg_logits, dn_logits = model(x)\n",
        "\n",
        "print(seg_logits.shape)            # [1, 1, 640, 640]\n",
        "print(dn_logits.shape)             # [1, 2]\n",
        "print(\"gt:\", dn, \"pred:\", torch.argmax(dn_logits, dim=1).item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collate function for tri-task training: batch images and masks, pack variable-length YOLO boxes into a YOLO-compatible batch dict, and return day/night labels separately so they don‚Äôt interfere with YOLO‚Äôs loss computation."
      ],
      "metadata": {
        "id": "NXsPKx8yH4Hz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLRY2SDJA9wq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_det_seg(batch):\n",
        "    # batch items: (img, labels, mask, dn)\n",
        "    imgs, labels_list, masks, dns = zip(*batch)\n",
        "\n",
        "    imgs = torch.stack(imgs, 0)         # [B,3,H,W]\n",
        "    masks = torch.stack(masks, 0)       # [B,1,H,W]\n",
        "    dn = torch.tensor(dns, dtype=torch.long)  # [B]\n",
        "\n",
        "    bboxes_all, cls_all, batch_idx_all = [], [], []\n",
        "    for i, lab in enumerate(labels_list):\n",
        "        if lab.numel() == 0:\n",
        "            continue\n",
        "        cls = lab[:, 0:1].long()\n",
        "        bboxes = lab[:, 1:5].float()\n",
        "        bboxes_all.append(bboxes)\n",
        "        cls_all.append(cls)\n",
        "        batch_idx_all.append(torch.full((lab.shape[0],), i, dtype=torch.long))\n",
        "\n",
        "    if len(bboxes_all):\n",
        "        bboxes = torch.cat(bboxes_all, 0)\n",
        "        cls = torch.cat(cls_all, 0)\n",
        "        batch_idx = torch.cat(batch_idx_all, 0)\n",
        "    else:\n",
        "        bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "        cls = torch.zeros((0, 1), dtype=torch.long)\n",
        "        batch_idx = torch.zeros((0,), dtype=torch.long)\n",
        "\n",
        "\n",
        "    yolo_batch = {\"img\": imgs, \"bboxes\": bboxes, \"cls\": cls, \"batch_idx\": batch_idx}\n",
        "\n",
        "    # return extras separately (so YOLO loss doesn't see them)\n",
        "    return yolo_batch, masks, dn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRaZRWltA9y1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device, scaler, lambda_seg=1, lambda_dn=0.1):\n",
        "    model.train()\n",
        "    tot_loss = tot_det = tot_seg = tot_dn = 0.0\n",
        "\n",
        "    for det_batch, mask, dn in loader:\n",
        "        det_batch = {k: v.to(device, non_blocking=True) for k, v in det_batch.items()}\n",
        "        mask = mask.to(device, non_blocking=True)\n",
        "        dn = dn.to(device, non_blocking=True).long()\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            det_loss, _, seg_logits, dn_logits = model(det_batch)\n",
        "            det_loss = det_loss.mean()\n",
        "\n",
        "            seg_loss = F.binary_cross_entropy_with_logits(seg_logits, mask)\n",
        "\n",
        "\n",
        "            valid = (dn >= 0)\n",
        "            dn_loss = F.cross_entropy(dn_logits[valid], dn[valid]) if valid.any() else torch.zeros((), device=device)\n",
        "\n",
        "            loss = det_loss + lambda_seg * seg_loss + lambda_dn * dn_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        tot_loss += loss.item()\n",
        "        tot_det  += det_loss.item()\n",
        "        tot_seg  += seg_loss.item()\n",
        "        tot_dn   += dn_loss.item()\n",
        "\n",
        "    n = max(1, len(loader))\n",
        "    return (tot_loss / n), (tot_det / n), (tot_seg / n), (tot_dn / n)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute mean validation IoU for drivable-area segmentation in the tri-task setup by running the model on YOLO-style batches and comparing thresholded segmentation predictions against ground-truth masks (optionally on a limited number of batches)."
      ],
      "metadata": {
        "id": "UJU9epgsw0nL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XKvHjKVD9Sv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_iou(model, loader, device, max_batches=None):\n",
        "    model.eval()\n",
        "    total_iou = 0.0\n",
        "    total_imgs = 0\n",
        "\n",
        "    for bi, (det_batch, mask, dn) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bi >= max_batches):\n",
        "            break\n",
        "\n",
        "        det_batch = {k: v.to(device, non_blocking=True) for k, v in det_batch.items()}\n",
        "        gt = (mask.to(device, non_blocking=True) > 0.5).float()\n",
        "\n",
        "        det_loss, det_items, seg_logits, dn_logits = model(det_batch)  # dict-path returns 4\n",
        "        pred = (torch.sigmoid(seg_logits) > 0.5).float()\n",
        "\n",
        "        inter = (pred * gt).sum(dim=(1, 2, 3))\n",
        "        union = ((pred + gt) > 0).float().sum(dim=(1, 2, 3)).clamp_min(1.0)\n",
        "\n",
        "        iou = inter / union\n",
        "        total_iou += iou.sum().item()\n",
        "        total_imgs += iou.numel()\n",
        "\n",
        "    return total_iou / max(1, total_imgs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the tri-task training dataset (images, YOLO labels, drivable masks, and day/night labels from CSV)."
      ],
      "metadata": {
        "id": "SJ0mk5--xCjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kbjzh-yD9U9"
      },
      "outputs": [],
      "source": [
        "train_ds = BDDDetDrivableDataset(\n",
        "    yolo_root=\"/content/BDD100K_640/yolo_640\",\n",
        "    mask_root=\"/content/BDD100K_640/drivable_masks_640\",\n",
        "    split=\"train\",\n",
        "    imgsz=640,\n",
        "    dn_csv_path=\"/content/daynight_labels.csv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tri-task training DataLoader using the custom collate function that returns a YOLO batch dict plus segmentation masks and day/night labels."
      ],
      "metadata": {
        "id": "XESHeF-kySTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTAUB2zPEXgw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        "    persistent_workers=False,\n",
        "    collate_fn=collate_det_seg\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the tri-task CBAM wrapper with the Ultralytics trainer model, re-register hooks, then load the pretrained det+seg checkpoint while skipping incompatible keys (new day/night head and the redesigned multi-scale cbam_neck) so training can continue with the new heads/modules."
      ],
      "metadata": {
        "id": "ZyIRL5zezFO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAv1uJquEXjf",
        "outputId": "765d3338-e610-4b83-d8a1-ca9ae4e9ddc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.6 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/BDD100K_640/yolo_640/dataset_640.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 24.5MB/s 0.0s\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "missing: ['dn_head.2.weight', 'dn_head.2.bias', 'cbam_neck.0.ca.mlp.0.weight', 'cbam_neck.0.ca.mlp.2.weight', 'cbam_neck.0.sa.conv.weight', 'cbam_neck.1.ca.mlp.0.weight', 'cbam_neck.1.ca.mlp.2.weight', 'cbam_neck.1.sa.conv.weight', 'cbam_neck.2.ca.mlp.0.weight', 'cbam_neck.2.ca.mlp.2.weight', 'cbam_neck.2.sa.conv.weight']\n",
            "unexpected: []\n",
            "OK: loaded Stage-A (det+seg) weights. cbam_neck will be randomly init for new 3-scale design.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch, copy\n",
        "from ultralytics.models.yolo.detect.train import DetectionTrainer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "stageA_pt   = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_cbam_640/weights/best.pt\"\n",
        "det_base_pt = \"/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt\"\n",
        "\n",
        "data_yaml = \"/content/BDD100K_640/yolo_640/dataset_640.yaml\"\n",
        "imgsz = 640\n",
        "\n",
        "# wrapper\n",
        "model = YOLOv8DetSemSeg(yolo_weights=det_base_pt, use_cbam=True).to(device)\n",
        "\n",
        "# trainer\n",
        "trainer = DetectionTrainer(overrides={\n",
        "    \"model\": det_base_pt,\n",
        "    \"data\":  data_yaml,\n",
        "    \"imgsz\": imgsz,\n",
        "    \"device\": 0,\n",
        "})\n",
        "trainer.setup_model()\n",
        "trainer.model.args = trainer.args\n",
        "trainer.model.init_criterion()\n",
        "\n",
        "# swap\n",
        "model.yolo = trainer.model.to(device)\n",
        "\n",
        "# re-hook\n",
        "model._neck_feats = None\n",
        "model.cbam_backbone = None\n",
        "model.cbam_neck = None\n",
        "model._register_backbone_hook_point1()\n",
        "model._register_detect_input_hook()\n",
        "\n",
        "# materialize Lazy + CBAM modules once\n",
        "with torch.no_grad():\n",
        "    _ = model(torch.zeros(1, 3, imgsz, imgsz, device=device))\n",
        "\n",
        "# load  weights but drop incompatible keys\n",
        "ckpt = torch.load(stageA_pt, map_location=device)\n",
        "sd = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
        "sd2 = copy.deepcopy(sd)\n",
        "\n",
        "# drop dn head\n",
        "sd2.pop(\"dn_head.2.weight\", None)\n",
        "sd2.pop(\"dn_head.2.bias\", None)\n",
        "\n",
        "# drop ALL old cbam_neck.* weights (they don't match new per-scale CBAM channels)\n",
        "for k in list(sd2.keys()):\n",
        "    if k.startswith(\"cbam_neck.\"):\n",
        "        sd2.pop(k)\n",
        "\n",
        "# load remaining weights\n",
        "missing, unexpected = model.load_state_dict(sd2, strict=False)\n",
        "print(\"missing:\", missing)\n",
        "print(\"unexpected:\", unexpected)\n",
        "\n",
        "print(\"OK: loaded Stage-A (det+seg) weights. cbam_neck will be randomly init for new 3-scale design.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity-check the tri-task model inference interface: confirm it returns three outputs (detection predictions, segmentation logits [1,1,640,640], and day/night logits [1,2])."
      ],
      "metadata": {
        "id": "n-0TalztzWuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jl0pz-4EXpD",
        "outputId": "d553c612-5590-4d61-efbb-acfa2007e7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 [<class 'dict'>, torch.Size([1, 1, 640, 640]), torch.Size([1, 2])]\n"
          ]
        }
      ],
      "source": [
        "# test: inference outputs count\n",
        "x = torch.randn(1, 3, 640, 640, device=device)\n",
        "out = model(x)\n",
        "print(len(out), [o.shape if torch.is_tensor(o) else type(o) for o in out])\n",
        "# 3 outputs, and dn_logits shape [1,2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an optimizer that updates only the day/night classification head (dn_head)."
      ],
      "metadata": {
        "id": "pEurlUf7zgLM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrzTrYcBFizq"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.dn_head.parameters() if p.requires_grad],\n",
        "    lr=1e-4,\n",
        "    weight_decay=5e-4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the validation dataset and DataLoader for the tri-task setup (including day/night labels) using the same collate function as training."
      ],
      "metadata": {
        "id": "V-4Wober0vC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4yESs05F4Zq"
      },
      "outputs": [],
      "source": [
        "val_ds = BDDDetDrivableDataset(\n",
        "    yolo_root=\"/content/BDD100K_640/yolo_640\",\n",
        "    mask_root=\"/content/BDD100K_640/drivable_masks_640\",\n",
        "    split=\"val\",\n",
        "    imgsz=640,\n",
        "    dn_csv_path=\"/content/daynight_labels.csv\"\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        "    persistent_workers=False,\n",
        "    collate_fn=collate_det_seg\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity-check the tri-task wrapper with CBAM disabled vs enabled, confirming both configurations produce segmentation logits [1,1,640,640] and day/night logits [1,2] in inference."
      ],
      "metadata": {
        "id": "xlGUJyeV1COQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1WH9L8BGNL8",
        "outputId": "c43e3ed1-969c-4085-f85f-8d9b23e8246d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OFF seg: torch.Size([1, 1, 640, 640]) dn: torch.Size([1, 2])\n",
            "ON  seg: torch.Size([1, 1, 640, 640]) dn: torch.Size([1, 2])\n"
          ]
        }
      ],
      "source": [
        "best_pt = \"/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "img, labels, mask, dn = ds[0]\n",
        "x = img.unsqueeze(0).to(device)\n",
        "\n",
        "# CBAM OFF\n",
        "m0 = YOLOv8DetSemSeg(best_pt, use_cbam=False).to(device).eval()\n",
        "with torch.no_grad():\n",
        "    det0, seg0, dn0 = m0(x)\n",
        "print(\"OFF seg:\", seg0.shape, \"dn:\", dn0.shape)\n",
        "\n",
        "# CBAM ON\n",
        "m1 = YOLOv8DetSemSeg(best_pt, use_cbam=True).to(device).eval()\n",
        "with torch.no_grad():\n",
        "    det1, seg1, dn1 = m1(x)\n",
        "print(\"ON  seg:\", seg1.shape, \"dn:\", dn1.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-batch sanity run for day/night training only: freeze all parameters except dn_head, run a single training batch with AMP, and print the cross-entropy loss and accuracy (loss=0.706, acc=0.375, logits shape (8,2)) to confirm the head-only update works."
      ],
      "metadata": {
        "id": "YFhOJAkG1eiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-epoch sanity test for dn only\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# freeze all\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "# unfreeze dn head only\n",
        "for p in model.dn_head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.dn_head.parameters() if p.requires_grad],\n",
        "    lr=1e-4, weight_decay=5e-4\n",
        ")\n",
        "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "model.train()\n",
        "tot_loss, tot_acc, nb = 0.0, 0.0, 0\n",
        "\n",
        "for det_batch, mask, dn in train_loader:  # collate_det_seg -> (det_batch, mask, dn)\n",
        "    img = det_batch[\"img\"].to(device, non_blocking=True)\n",
        "    dn  = dn.to(device, non_blocking=True).long()\n",
        "\n",
        "    valid = (dn >= 0)\n",
        "    if not valid.any():\n",
        "        continue\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.amp.autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
        "        out = model(img)             # returns (det, seg, dn) in the model\n",
        "        dn_logits = out[-1]          # last output\n",
        "        loss = F.cross_entropy(dn_logits[valid], dn[valid])\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(dn_logits[valid], dim=1)\n",
        "        acc = (pred == dn[valid]).float().mean().item()\n",
        "\n",
        "    tot_loss += loss.item()\n",
        "    tot_acc += acc\n",
        "    nb += 1\n",
        "\n",
        "    print(f\"ONE BATCH OK | loss={loss.item():.4f} acc={acc:.4f} | dn_logits={tuple(dn_logits.shape)}\")\n",
        "    break  # stop after 1 batch\n",
        "\n",
        "print(f\"Sanity 1-epoch (1 batch) done | avg_loss={tot_loss/max(1,nb):.4f} avg_acc={tot_acc/max(1,nb):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7v8oUa5Tj4h",
        "outputId": "53432fdc-7179-4fd3-8324-ec7a216663a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONE BATCH OK | loss=0.7060 acc=0.3750 | dn_logits=(8, 2)\n",
            "Sanity 1-epoch (1 batch) done | avg_loss=0.7060 avg_acc=0.3750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the tri-task model‚Äôs day/night tagging head only for 30 epochs: freeze all other parameters, optimize cross-entropy on day/night labels with AMP, evaluate validation accuracy every 2 epochs, and save snapshots plus best/last checkpoints for resume.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lv9lZknm2rUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcZWnsdcWVlV",
        "outputId": "3f67529e-ad79-4467-a589-9b16cc7dbca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01/30 | train_dn_loss=0.6710 train_dn_acc=0.5677 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch02.pt\n",
            "epoch 02/30 | train_dn_loss=0.6441 train_dn_acc=0.5919 | val_dn_acc=0.6470\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.6470)\n",
            "epoch 03/30 | train_dn_loss=0.6200 train_dn_acc=0.6796 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch04.pt\n",
            "epoch 04/30 | train_dn_loss=0.5990 train_dn_acc=0.7419 | val_dn_acc=0.7430\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.7430)\n",
            "epoch 05/30 | train_dn_loss=0.5796 train_dn_acc=0.7752 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch06.pt\n",
            "epoch 06/30 | train_dn_loss=0.5613 train_dn_acc=0.8005 | val_dn_acc=0.8390\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.8390)\n",
            "epoch 07/30 | train_dn_loss=0.5462 train_dn_acc=0.8153 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch08.pt\n",
            "epoch 08/30 | train_dn_loss=0.5322 train_dn_acc=0.8236 | val_dn_acc=0.8670\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.8670)\n",
            "epoch 09/30 | train_dn_loss=0.5193 train_dn_acc=0.8308 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch10.pt\n",
            "epoch 10/30 | train_dn_loss=0.5072 train_dn_acc=0.8385 | val_dn_acc=0.8760\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.8760)\n",
            "epoch 11/30 | train_dn_loss=0.4965 train_dn_acc=0.8414 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch12.pt\n",
            "epoch 12/30 | train_dn_loss=0.4861 train_dn_acc=0.8463 | val_dn_acc=0.8860\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.8860)\n",
            "epoch 13/30 | train_dn_loss=0.4769 train_dn_acc=0.8490 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch14.pt\n",
            "epoch 14/30 | train_dn_loss=0.4675 train_dn_acc=0.8533 | val_dn_acc=0.8890\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.8890)\n",
            "epoch 15/30 | train_dn_loss=0.4601 train_dn_acc=0.8557 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch16.pt\n",
            "epoch 16/30 | train_dn_loss=0.4520 train_dn_acc=0.8577 | val_dn_acc=0.8780\n",
            "epoch 17/30 | train_dn_loss=0.4444 train_dn_acc=0.8624 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch18.pt\n",
            "epoch 18/30 | train_dn_loss=0.4376 train_dn_acc=0.8649 | val_dn_acc=0.8940\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.8940)\n",
            "epoch 19/30 | train_dn_loss=0.4313 train_dn_acc=0.8656 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch20.pt\n",
            "epoch 20/30 | train_dn_loss=0.4246 train_dn_acc=0.8691 | val_dn_acc=0.8860\n",
            "epoch 21/30 | train_dn_loss=0.4200 train_dn_acc=0.8701 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch22.pt\n",
            "epoch 22/30 | train_dn_loss=0.4146 train_dn_acc=0.8728 | val_dn_acc=0.8910\n",
            "epoch 23/30 | train_dn_loss=0.4098 train_dn_acc=0.8735 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch24.pt\n",
            "epoch 24/30 | train_dn_loss=0.4035 train_dn_acc=0.8759 | val_dn_acc=0.9010\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.9010)\n",
            "epoch 25/30 | train_dn_loss=0.3986 train_dn_acc=0.8789 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch26.pt\n",
            "epoch 26/30 | train_dn_loss=0.3950 train_dn_acc=0.8785 | val_dn_acc=0.8970\n",
            "epoch 27/30 | train_dn_loss=0.3897 train_dn_acc=0.8803 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch28.pt\n",
            "epoch 28/30 | train_dn_loss=0.3871 train_dn_acc=0.8801 | val_dn_acc=0.9130\n",
            "  saved BEST: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt (best_dn_acc=0.9130)\n",
            "epoch 29/30 | train_dn_loss=0.3830 train_dn_acc=0.8826 | val_dn_acc=skip\n",
            "  saved SNAPSHOT: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/epoch30.pt\n",
            "epoch 30/30 | train_dn_loss=0.3783 train_dn_acc=0.8850 | val_dn_acc=0.9090\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# train day/night only\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "save_root = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights\"\n",
        "os.makedirs(save_root, exist_ok=True)\n",
        "best_path = os.path.join(save_root, \"best.pt\")\n",
        "ckpt_path = os.path.join(save_root, \"last.ckpt\")\n",
        "\n",
        "# freezing everything\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Unfreezing only tagging head\n",
        "for p in model.dn_head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Optimizer (only dn_head)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.dn_head.parameters() if p.requires_grad],\n",
        "    lr=1e-4,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# AMP scalar\n",
        "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# Resume\n",
        "start_epoch = 1\n",
        "best_dn_acc = -1.0\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"], strict=False)\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    best_dn_acc = ckpt.get(\"best_dn_acc\", -1.0)\n",
        "    if \"scaler\" in ckpt:\n",
        "        scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "    start_epoch = int(ckpt[\"epoch\"]) + 1\n",
        "    print(f\"Resuming from epoch {start_epoch} | best_dn_acc={best_dn_acc:.4f}\", flush=True)\n",
        "\n",
        "# train Oone epoch (DN only, no det/seg loss)\n",
        "def train_one_epoch_dn_only(model, loader, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "    tot_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for det_batch, mask, dn in loader:   # collate_det_seg returns (det_batch, mask, dn)\n",
        "        img = det_batch[\"img\"].to(device, non_blocking=True)\n",
        "        dn  = dn.to(device, non_blocking=True).long()\n",
        "\n",
        "        valid = (dn >= 0)\n",
        "        if not valid.any():\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            out = model(img)  # inference-path: returns (det_preds, seg_logits, dn_logits) or (det_preds, seg_logits)\n",
        "            dn_logits = out[-1]  # last output (model returns dn_logits last)\n",
        "            dn_loss = F.cross_entropy(dn_logits[valid], dn[valid])\n",
        "\n",
        "        scaler.scale(dn_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = torch.argmax(dn_logits[valid], dim=1)\n",
        "            acc = (pred == dn[valid]).float().mean().item()\n",
        "\n",
        "        tot_loss += dn_loss.item()\n",
        "        tot_acc += acc\n",
        "        n_batches += 1\n",
        "\n",
        "    n = max(1, n_batches)\n",
        "    return tot_loss / n, tot_acc / n\n",
        "\n",
        "# val DN accuracy\n",
        "@torch.no_grad()\n",
        "def val_dn_acc(model, loader, device, max_batches=None):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for bi, (det_batch, mask, dn) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bi >= max_batches):\n",
        "            break\n",
        "\n",
        "        img = det_batch[\"img\"].to(device, non_blocking=True)\n",
        "        dn  = dn.to(device, non_blocking=True).long()\n",
        "\n",
        "        valid = (dn >= 0)\n",
        "        if not valid.any():\n",
        "            continue\n",
        "\n",
        "        out = model(img)\n",
        "        dn_logits = out[-1]\n",
        "        pred = torch.argmax(dn_logits[valid], dim=1)\n",
        "\n",
        "        correct += (pred == dn[valid]).sum().item()\n",
        "        total += valid.sum().item()\n",
        "\n",
        "    return correct / max(1, total)\n",
        "\n",
        "# Train loop\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(start_epoch, epochs + 1):\n",
        "    train_dn_loss, train_dn_acc = train_one_epoch_dn_only(\n",
        "        model=model,\n",
        "        loader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        scaler=scaler\n",
        "    )\n",
        "\n",
        "    # validate every 2 epochs\n",
        "    if epoch % 2 == 0:\n",
        "        dn_acc = val_dn_acc(model, val_loader, device, max_batches=125)\n",
        "\n",
        "        snap_path = os.path.join(save_root, f\"epoch{epoch:02d}.pt\")\n",
        "        torch.save(model.state_dict(), snap_path)\n",
        "        print(f\"  saved SNAPSHOT: {snap_path}\", flush=True)\n",
        "\n",
        "        print(\n",
        "            f\"epoch {epoch:02d}/{epochs} | \"\n",
        "            f\"train_dn_loss={train_dn_loss:.4f} train_dn_acc={train_dn_acc:.4f} | \"\n",
        "            f\"val_dn_acc={dn_acc:.4f}\",\n",
        "            flush=True\n",
        "        )\n",
        "\n",
        "        # save BEST by dn accuracy\n",
        "        if dn_acc > best_dn_acc:\n",
        "            best_dn_acc = dn_acc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(f\"  saved BEST: {best_path} (best_dn_acc={best_dn_acc:.4f})\", flush=True)\n",
        "    else:\n",
        "        print(\n",
        "            f\"epoch {epoch:02d}/{epochs} | \"\n",
        "            f\"train_dn_loss={train_dn_loss:.4f} train_dn_acc={train_dn_acc:.4f} | \"\n",
        "            f\"val_dn_acc=skip\",\n",
        "            flush=True\n",
        "        )\n",
        "\n",
        "    # save LAST (resume checkpoint)\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"best_dn_acc\": best_dn_acc,\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "    }, ckpt_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final tri-task evaluation script: load the best CBAM tri-task checkpoint into the YOLOv8 wrapper (with trainer swap and hooks), evaluate detection with Ultralytics val() on validation and test splits, and compute custom segmentation mean IoU plus day/night accuracy, balanced accuracy, F1, and confusion counts on validation and test loaders."
      ],
      "metadata": {
        "id": "Q94z4ssw3_xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL TRI-TASK EVALUATION\n",
        "# Detection: Ultralytics .val() on val + test\n",
        "# Segmentation: mean IoU on val + test (binary)\n",
        "# Day/Night: acc + balanced acc + F1 on val + test\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.models.yolo.detect.train import DetectionTrainer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dev_override = 0 if device == \"cuda\" else \"cpu\"\n",
        "\n",
        "# paths\n",
        "data_yaml = \"/content/BDD100K_640/yolo_640/dataset_640.yaml\"\n",
        "\n",
        "best_wrapper_pt = \"/content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt\"\n",
        "\n",
        "det_base_pt = \"/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt\"\n",
        "\n",
        "imgsz = 640\n",
        "batch = 8\n",
        "\n",
        "\n",
        "# 1) build model (wrapper + trainer swap) + load best_wrapper_pt\n",
        "# =========================\n",
        "model = YOLOv8DetSemSeg(yolo_weights=det_base_pt, use_cbam=True).to(device)\n",
        "\n",
        "trainer = DetectionTrainer(overrides={\n",
        "    \"model\": det_base_pt,\n",
        "    \"data\":  data_yaml,\n",
        "    \"imgsz\": imgsz,\n",
        "    \"device\": dev_override,\n",
        "    \"batch\": batch,\n",
        "})\n",
        "trainer.setup_model()\n",
        "trainer.model.args = trainer.args\n",
        "trainer.model.init_criterion()\n",
        "\n",
        "# swap YOLO model into wrapper\n",
        "model.yolo = trainer.model.to(device)\n",
        "\n",
        "# re-hook\n",
        "model._neck_feats = None\n",
        "model.cbam_backbone = None\n",
        "model.cbam_neck = None\n",
        "model._register_backbone_hook_point1()\n",
        "model._register_detect_input_hook()\n",
        "\n",
        "# materialize Lazy + CBAM\n",
        "with torch.no_grad():\n",
        "    _ = model(torch.zeros(1, 3, imgsz, imgsz, device=device))\n",
        "\n",
        "# load wrapper state_dict (non-strict because cbam_neck.0/1/2 + dn_head may differ)\n",
        "sd = torch.load(best_wrapper_pt, map_location=device)\n",
        "if isinstance(sd, dict) and \"state_dict\" in sd:\n",
        "    sd = sd[\"state_dict\"]\n",
        "model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "\n",
        "print(\"OK: loaded wrapper best:\", best_wrapper_pt)\n",
        "\n",
        "\n",
        "# 2) Detection eval (Ultralytics)\n",
        "\n",
        "# attach trained detection model into a YOLO() wrapper and run .val()\n",
        "yolo = YOLO(det_base_pt)\n",
        "yolo.model = model.yolo  # use the swapped, trained model\n",
        "\n",
        "print(\"\\n=== DETECTION: VAL ===\")\n",
        "det_val = yolo.val(data=data_yaml, imgsz=imgsz, device=dev_override, split=\"val\", batch=batch)\n",
        "\n",
        "print(\"\\n=== DETECTION: TEST ===\")\n",
        "try:\n",
        "    det_test = yolo.val(data=data_yaml, imgsz=imgsz, device=dev_override, split=\"test\", batch=batch)\n",
        "except Exception as e:\n",
        "    det_test = None\n",
        "    print(\"No test split or error running test val():\", repr(e))\n",
        "\n",
        "# 3) deg + day/night eval\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_seg_dn(model, loader, device, max_batches=None):\n",
        "    model.eval()\n",
        "\n",
        "    # seg IoU\n",
        "    inter_sum = 0.0\n",
        "    union_sum = 0.0\n",
        "\n",
        "    # dn metrics (ignore dn=-1)\n",
        "    tp = tn = fp = fn = 0\n",
        "\n",
        "    n_batches = 0\n",
        "    for bi, (det_batch, mask, dn) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bi >= max_batches):\n",
        "            break\n",
        "\n",
        "        img = det_batch[\"img\"].to(device, non_blocking=True)\n",
        "        gt  = (mask.to(device, non_blocking=True) > 0.5).float()\n",
        "        dn  = dn.to(device, non_blocking=True).long()\n",
        "\n",
        "        out = model(img)            # (det, seg, dn)\n",
        "        seg_logits = out[1]\n",
        "        dn_logits  = out[-1]\n",
        "\n",
        "        pred = (torch.sigmoid(seg_logits) > 0.5).float()\n",
        "\n",
        "        inter = (pred * gt).sum(dim=(1,2,3))\n",
        "        union = ((pred + gt) > 0).float().sum(dim=(1,2,3))\n",
        "        inter_sum += inter.sum().item()\n",
        "        union_sum += union.sum().item()\n",
        "\n",
        "        valid = (dn >= 0)\n",
        "        if valid.any():\n",
        "            pred_dn = torch.argmax(dn_logits[valid], dim=1)\n",
        "            true_dn = dn[valid]\n",
        "\n",
        "            # positive class = 1 (night), negative = 0 (day)\n",
        "            tp += int(((pred_dn == 1) & (true_dn == 1)).sum().item())\n",
        "            tn += int(((pred_dn == 0) & (true_dn == 0)).sum().item())\n",
        "            fp += int(((pred_dn == 1) & (true_dn == 0)).sum().item())\n",
        "            fn += int(((pred_dn == 0) & (true_dn == 1)).sum().item())\n",
        "\n",
        "        n_batches += 1\n",
        "\n",
        "    miou = (inter_sum / max(1.0, union_sum))\n",
        "\n",
        "    # dn acc/balanced acc/f1\n",
        "    total = tp + tn + fp + fn\n",
        "    acc = (tp + tn) / max(1, total)\n",
        "\n",
        "    tpr = tp / max(1, (tp + fn))  # recall night\n",
        "    tnr = tn / max(1, (tn + fp))  # recall day\n",
        "    bal_acc = 0.5 * (tpr + tnr)\n",
        "\n",
        "    prec = tp / max(1, (tp + fp))\n",
        "    rec  = tpr\n",
        "    f1 = (2 * prec * rec) / max(1e-12, (prec + rec))\n",
        "\n",
        "    cm = {\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp}\n",
        "    return miou, acc, bal_acc, f1, cm\n",
        "\n",
        "\n",
        "print(\"\\n=== SEG + DAY/NIGHT: VAL (custom) ===\")\n",
        "val_miou, val_acc, val_bal, val_f1, val_cm = eval_seg_dn(model, val_loader, device)\n",
        "print(f\"SEG mean IoU: {val_miou:.4f}\")\n",
        "print(f\"DN acc: {val_acc:.4f} | balanced acc: {val_bal:.4f} | F1(night=1): {val_f1:.4f}\")\n",
        "print(\"DN confusion (tn,fp,fn,tp):\", val_cm)\n",
        "\n",
        "print(\"\\n=== SEG + DAY/NIGHT: TEST (custom) ===\")\n",
        "try:\n",
        "    test_miou, test_acc, test_bal, test_f1, test_cm = eval_seg_dn(model, test_loader, device)\n",
        "    print(f\"SEG mean IoU: {test_miou:.4f}\")\n",
        "    print(f\"DN acc: {test_acc:.4f} | balanced acc: {test_bal:.4f} | F1(night=1): {test_f1:.4f}\")\n",
        "    print(\"DN confusion (tn,fp,fn,tp):\", test_cm)\n",
        "except NameError:\n",
        "    print(\"No test_loader defined. (If you have a test split, build test_ds/test_loader like val_loader.)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn8M5Oze5hqZ",
        "outputId": "fc5db54e-2d5b-47bf-c261-972753d66f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.6 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/BDD100K_640/yolo_640/dataset_640.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/XAI_Project/experiments/det_baseline/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,011,823 parameters, 3,011,807 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "OK: loaded wrapper best: /content/drive/MyDrive/XAI_Project/experiments/det_seg_dn_cbam_640/weights/best.pt\n",
            "\n",
            "=== DETECTION: VAL ===\n",
            "Ultralytics 8.4.6 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 73 layers, 3,006,623 parameters, 13,455 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1449.7¬±545.1 MB/s, size: 49.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/BDD100K_640/yolo_640/val/labels... 10000 images, 80 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10000/10000 2.2Kit/s 4.5s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/BDD100K_640/yolo_640/val/labels.cache\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1250/1250 11.8it/s 1:46\n",
            "                   all      10000     122617      0.615      0.446      0.481      0.281\n",
            "                     0       9879     102506      0.738      0.672      0.723      0.426\n",
            "                     1       2689       4245      0.553      0.464       0.46      0.306\n",
            "                     2       1242       1597      0.592      0.423      0.463      0.333\n",
            "                     3       3220      13262      0.681       0.42      0.484      0.217\n",
            "                     4        578       1007      0.511      0.251      0.276       0.12\n",
            "Speed: 1.1ms preprocess, 3.9ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val\u001b[0m\n",
            "\n",
            "=== DETECTION: TEST ===\n",
            "Ultralytics 8.4.6 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 26.3¬±7.4 MB/s, size: 56.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/BDD100K_640/yolo_640/test/labels... 20000 images, 143 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20000/20000 613.2it/s 32.6s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/BDD100K_640/yolo_640/test/images/e6f10c58-c46de527.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/BDD100K_640/yolo_640/test/labels.cache\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2500/2500 12.0it/s 3:28\n",
            "                   all      20000     243649      0.607      0.437      0.466       0.27\n",
            "                     0      19776     205093      0.739      0.667      0.719      0.425\n",
            "                     1       5500       8701      0.567      0.457      0.457      0.303\n",
            "                     2       2459       3217      0.562      0.394      0.404      0.287\n",
            "                     3       6213      24641      0.675      0.409      0.479      0.219\n",
            "                     4       1182       1997      0.493      0.258      0.272      0.115\n",
            "Speed: 1.0ms preprocess, 3.9ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val2\u001b[0m\n",
            "\n",
            "=== SEG + DAY/NIGHT: VAL (custom) ===\n",
            "SEG mean IoU: 0.7152\n",
            "DN acc: 0.9180 | balanced acc: 0.9108 | F1(night=1): 0.8998\n",
            "DN confusion (tn,fp,fn,tp): {'tn': 5052, 'fp': 206, 'fn': 547, 'tp': 3382}\n",
            "\n",
            "=== SEG + DAY/NIGHT: TEST (custom) ===\n",
            "No test_loader defined. (If you have a test split, build test_ds/test_loader like val_loader.)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPUOyrQIIYGK4+wQS6jMauU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}